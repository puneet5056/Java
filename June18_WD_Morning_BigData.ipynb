{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "June18-WD-Morning-BigData",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puneet5056/Java/blob/master/June18_WD_Morning_BigData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "pR9M_8dfaqYb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Hi welcome\n",
        "\n",
        "CREATE EXTERNAL TABLE `emp_csv`(\n",
        "  `id` string COMMENT 'from deserializer',\n",
        "  `name` string COMMENT 'from deserializer',\n",
        "  `dept` string COMMENT 'from deserializer',\n",
        "  `joindate` string COMMENT 'from deserializer',\n",
        "  `salary` string COMMENT 'from deserializer')\n",
        "ROW FORMAT SERDE\n",
        "  'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
        "WITH SERDEPROPERTIES (\n",
        "  'escapeChar'='\\\\',\n",
        "  'quoteChar'=\"\\\"\",\n",
        "  'separatorChar'=',')\n",
        "STORED AS INPUTFORMAT\n",
        "  'org.apache.hadoop.mapred.TextInputFormat'\n",
        "OUTPUTFORMAT\n",
        "  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
        "LOCATION\n",
        "  'hdfs://localhost:9000/hive/practice/emp'\n",
        "TBLPROPERTIES (\n",
        "  'transient_lastDdlTime'='1536548978')\n",
        "\n",
        "\n",
        "1,stu1,dept1,2010-01-01,1000\n",
        "2,stu2,dept1,2010-01-01,2000\n",
        "3,stu3,dept1,2011-01-01,3000\n",
        "4,stu4,dept2,2011-01-01,500\n",
        "5,stu5,dept2,2012-01-01,500\n",
        "6,stu6,dept2,2012-01-01,500\n",
        "7,stu7,\"de,pt3\",2012-01-01,2000\n",
        "8,stu8,\"de,pt3\",2012-01-01,2000\n",
        "\n",
        "\n",
        "load data local inpath 'empdata' into table emp_csv;\n",
        "load data local inpath 'empdata' into table emp_csv;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bAiBxrun5MTw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S_egDZotbfvL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Hive Partitioning\n",
        "emp_csv as source \n",
        "Create a partitioned tble emp_part - good candidates for partitioning column are\n",
        "dept, year, month or fiscal week\n",
        "\n",
        "on emp id we can go for bucketing \n",
        "hive (hivepractice)> desc emp_csv\n",
        "hive (hivepractice)> select * from emp_csv;\n",
        "hive (hivepractice)> load data local inpath 'empdata' into table emp_csv;\n",
        "Time taken: 0.102 seconds, Fetched: 16 row(s)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e-XXy8M3dkDy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "create table emp_part with concat of year and month\n",
        "Create with delimiter as # field delimiter\n",
        "hive (hivepractice)> show create table emp_csv;\n",
        "CREATE EXTERNAL TABLE `emp_part`(\n",
        "  `id` string ,\n",
        "  `name` string ,\n",
        "  `dept` string ,\n",
        "  `joindate` string ,\n",
        "  `salary` string )\n",
        "partitioned by (fiscalmonth int)\n",
        "ROW FORMAT SERDE\n",
        "  'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
        "WITH SERDEPROPERTIES (\n",
        "  'escapeChar'='\\\\',\n",
        "  'quoteChar'=\"\\\"\",\n",
        "  'separatorChar'='#')\n",
        "stored as textfile;\n",
        "\n",
        "hive (hivepractice)> show create table emp_part;\n",
        "desc  emp_part;\n",
        "# Partition Information\n",
        "# col_name              data_type               comment\n",
        "\n",
        "fiscalmonth             int\n",
        "\n",
        "hive (hivepractice)> show partitions emp_part;\n",
        "\n",
        "hive (hivepractice)> dfs -ls /hive/practice/emp_part;\n",
        "\n",
        "\n",
        " select *, concat(year(joindate),month(joindate)) from emp_csv;\n",
        "\n",
        "   select if(month(\"2016-11-20\")<10,\n",
        "             concat(\"0\",month(\"2016-11-20\")),\n",
        "             month(\"2016-11-20\")) ;\n",
        "\n",
        "     select concat(year(joindate),                    \n",
        "             if(month(joindate)<10,\n",
        "             concat(\"0\",month(joindate)),\n",
        "             month(joindate))\n",
        "                  )  from emp_csv;\n",
        "\n",
        "      \n",
        " insert into table emp_part  partition(fiscalmonth) \n",
        "select * , \n",
        "concat(year(joindate), \n",
        " if(month(joindate)<10,\n",
        "    concat(\"0\",month(joindate)),\n",
        "    month(joindate)\n",
        "   )\n",
        "  )  from emp_csv;\n",
        "set hive.exec.dynamic.partition.mode=nonstrict;\n",
        "----------------------------\n",
        "--  insert into table emp_part  partition(fiscalmonth=201001) \n",
        "--  LOAD DATA LOCAL  INPATH 'emp1'  \n",
        "INTO TABLE emp_part PARTITION (fiscalmonth=201001)\n",
        "1,stu1,dept1,2010-01-01,1000\n",
        "2,stu2,dept1,2010-01-01,2000\n",
        "----------------------------\n",
        "\n",
        "-- insert overwrite affects only the partitions that are present in select query.\n",
        "-- other partitions are uneffected.\n",
        "\n",
        "insert overwrite  table emp_part  partition(fiscalmonth) \n",
        "select * , \n",
        "concat(year(joindate), \n",
        " if(month(joindate)<10,\n",
        "    concat(\"0\",month(joindate)),\n",
        "    month(joindate)\n",
        "   )\n",
        "  )  from emp_csv where concat(year(joindate), \n",
        " if(month(joindate)<10,\n",
        "    concat(\"0\",month(joindate)),\n",
        "    month(joindate)\n",
        "   )\n",
        "  )   =201001;\n",
        "-------------------------------------------------\n",
        "\n",
        "hive (hivepractice)> truncate table emp_part;\n",
        "FAILED: SemanticException [Error 10146]: Cannot truncate non-managed table emp_part.\n",
        "\n",
        "hive (hivepractice)> dfs -mkdir -p /hive/practice/emp_part/fiscalmonth=9999;\n",
        "hive (hivepractice)> msck repair table emp_part;\n",
        "hive (hivepractice)> alter table emp_part set TBLPROPERTIES (\"EXTERNAL\"=\"FALSE\")\n",
        "\n",
        "Partitions not in metastore:    emp_part:fiscalmonth=9999\n",
        "Partitions missing from filesystem:     emp_part:fiscalmonth=201001     emp_part:fiscalmonth=201101     emp_part:fiscalmonth=201201\n",
        "Repair: Added partition to metastore emp_part:fiscalmonth=9999\n",
        "\n",
        "    MSCK does not remove metadata, it only add metadata (if we created hdfs folder manullay)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZZ6LiDffozox",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Bucketing ::\n",
        "    \n",
        "    divide table data into fixed no of buckets (Files)\n",
        "    Hash partition\n",
        "    \n",
        "      [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]\n",
        "      \n",
        "      CREATE EXTERNAL TABLE `emp_bucketed`(\n",
        "  `id` string ,\n",
        "  `name` string ,\n",
        "  `dept` string ,\n",
        "  `joindate` string ,\n",
        "  `salary` string )\n",
        "      \n",
        "  clustered by (id) into 3 buckets\n",
        " ROW FORMAT SERDE\n",
        "  'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
        "WITH SERDEPROPERTIES (\n",
        "  'escapeChar'='\\\\',\n",
        "  'quoteChar'=\"\\\"\",\n",
        "  'separatorChar'='#')\n",
        "   \n",
        "      stored as textfile;\n",
        "\n",
        "    desc formatted emp_bucketed;\n",
        "    Num Buckets:            3\n",
        "    Bucket Columns:         [id]\n",
        "\n",
        "    \n",
        "    DML ???  - it is always insert overwrite in to bucketed table from non bucketed table.\n",
        "    \n",
        "    Location:               hdfs://localhost:9000/hive/practice/emp_bucketed\n",
        "\n",
        "    insert overwrite table emp_bucketed select * from emp_csv;\n",
        "    \n",
        "    \n",
        "    \n",
        "     CREATE EXTERNAL TABLE `emp_bucketed_sorted`(\n",
        "  `id` string ,\n",
        "  `name` string ,\n",
        "  `dept` string ,\n",
        "  `joindate` string ,\n",
        "  `salary` string )\n",
        "      \n",
        "  clustered by (id) sorted by (id) into 3 buckets\n",
        " ROW FORMAT SERDE\n",
        "  'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
        "WITH SERDEPROPERTIES (\n",
        "  'escapeChar'='\\\\',\n",
        "  'quoteChar'=\"\\\"\",\n",
        "  'separatorChar'='#') ;\n",
        "\n",
        "\n",
        "    insert overwrite table emp_bucketed_sorted select * from emp_csv;\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AnGTCSyXuyoP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Clauses in Select query\n",
        "Select distinct, *, col1, col2 , functions (col) - UDF, UDAF, UDTF, windown and analytic funcitons\n",
        "from A join B on A.col1=B.col2 \n",
        "join (select * from C) C1 on A.col2=C1.col3\n",
        "where A.col1 > 10  and  ( UDF(b.col2)=100 or C1.col3=15)\n",
        "group by A.col3 -- the columns not in group by clause have to be used with UDAFs in select query\n",
        "having UDAF(col4)>3  count(dept)>3 -- we can filter rows based on the result of aggregate funcitons in the same query\n",
        "orderby -- dont use -> global aggregation -> launch one reducer\n",
        "limit 10\n",
        "\n",
        "Actual execution is -> from, where, group by , having , limit , order by , select \n",
        "windowing and analytic functions evaluated , so we cannot filter data based on \n",
        "analytic funcitons within the same query - store it into table or make it as subquery , then you can apply where conditiin\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o1v0kXMtxZD_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "FUNCTIONS::\n",
        "    https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EICiZxzQzS0o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show functions;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-p4hxx0K5Nv7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Date : 13 Sept 2018\n",
        "  \n",
        "MOVIE LENS Analysis\n",
        "\n",
        "\n",
        "1. Top ten most viewed movies with their movies Name (Ascending or Descending order)\n",
        "2. Top twenty rated movies (Condition : The movie should be rated/viewed by at least 40 users)\n",
        "3. Top twenty rated movies (which is calculated in the previous step) with no of views in the\n",
        "following age group\n",
        "(Age group : 1. Young (<20 years),\n",
        "2. Young Adult(20-40 years),\n",
        "3.adult (> 40years) )\n",
        "(CROSS tab report we need to generate)\n",
        "movieid    20   20-40  40\n",
        "1       cnt      cnt   cnt\n",
        "2\n",
        "3\n",
        "\n",
        "4. Top ten critics (Users who have given very low ratings; Condition : The users should have at least\n",
        "rated 40 movies)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mkdir $HOME/Movielens_wd_morning\n",
        "cd  $HOME/Movielens_wd_morning\n",
        "wget https://github.com/dorababugjntup/Assignments/raw/master/3_MovieLens/ml-1m.zip\n",
        "sudo apt-get install unzip \n",
        "\n",
        "unzip ml-1m.zip\n",
        "\n",
        "\n",
        "\n",
        "Clean data sets:::\n",
        "      \n",
        "create a table with one column -> replace :: with : or #  - movie_datalake.ratings_ip \n",
        "create a external table with the columns -> hdpmovielens.ratings\n",
        "      \n",
        "  \n",
        "create database movie_datalake;\n",
        "create database hdpmovielens;\n",
        "\n",
        "set srcdb=movie_datalake;\n",
        "set targetdb=hdpmovielens;\n",
        "\n",
        "use movie_datalake;\n",
        "create table ratings_ip( line string)\n",
        "stored as textfile;\n",
        "\n",
        "/home/dorababugjntup/Movielens_wd_morning/ml-1m\n",
        "load data local inpath '${system:user.home}/Movielens_wd_morning/ml-1m/ratings.dat' into table ${hiveconf:srcdb}.ratings_ip;\n",
        "--insert into ratings   select split(line,\"::\")[0],split(line,\"::\")[1],split(line,\"::\")[2] from ratings_ip;\n",
        "\n",
        "select line from ratings_ip limit 10;\n",
        "\n",
        "\n",
        "insert overwrite directory \"/hive/movielens/ratings\"  select replace(line,\"::\",\":\") from ratings_ip ;\n",
        "create table hdpmovielens.ratings \n",
        "(userid int,\n",
        " movieid int ,\n",
        " rating int, \n",
        " createdtime bigint)\n",
        "row format delimited fields terminated by ':'\n",
        "stored as textfile\n",
        "location '/hive/movielens/ratings';\n",
        "\n",
        "\n",
        "Create Movie and User table, \n",
        "in movie table i want the movie released year as a column.\n",
        "\n",
        "load the data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LYv7ORHi56Hj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KCN1wpQ_MBPz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "create database movie_datalake;\n",
        "create database hdpmovielens;\n",
        "set srcdb=movie_datalake;\n",
        "set targetdb=hdpmovielens;\n",
        "use movie_datalake;\n",
        "create table ratings_ip( line string)\n",
        "stored as textfile;\n",
        "truncate table ratings_ip;\n",
        "select line from ratings_ip limit 10;\n",
        "load data local inpath '${system:user.home}/Movielens_wd_morning/ml-1m/ratings.dat' into table ${srcdb}.ratings_ip;\n",
        "set srcdb;\n",
        "load data local inpath '${system:user.home}/Movielens_wd_morning/ml-1m/ratings.dat' into table '${srcdb}'.ratings_ip;\n",
        "set srcdb;\n",
        "load data local inpath '${system:user.home}/Movielens_wd_morning/ml-1m/ratings.dat' into table ${hiveconf:srcdb}.ratings_ip;\n",
        "select line from ratings_ip limit 10;\n",
        "select count(1) from ratings_ip;\n",
        "select line from ratings_ip limit 10;\n",
        "desc function extended concat\n",
        ";\n",
        "SELECT concat('abc', 'def') ;\n",
        "SELECT concat('abc',\"===\", 'def') ;\n",
        "SELECT concat('abc',\"===\", 'def', \"===\", null) ;\n",
        "SELECT concat('abc',\"===\", 'def', \"===\", nvl( null, \"DEFAULT\")) ;\n",
        "desc function extended concat_ws;\n",
        "SELECT concat_ws(\"==\", 'abc', 'def',NULL) ;\n",
        "SELECT concat_ws(\"==\", 'abc', 'def',NULL, \"abcdedf\" ) ;\n",
        "SELECT concat( 'abc', 'def',NULL, \"abcdedf\" ) ;\n",
        "desc function extended concat_ws;\n",
        "select split(\"a b c d e\",\" \");\n",
        "select split(\"a b c d e\",\" \")[0];\n",
        "select split(\"a b c d e\",\" \")[2];\n",
        "select array(\"test\",\"test1\",\"test2\")\n",
        ";\n",
        "select array(\"test\",\"test1\",\"test2\")[2]\n",
        ";\n",
        "select array(\"test\",\"test1\",\"test2\")[3]\n",
        ";\n",
        "select array(\"test\",\"test1\",\"test2\")[100000]\n",
        ";\n",
        "select map(\"maths\",90,\"science\",100);\n",
        "select map_keys(map(\"maths\",90,\"science\",100));\n",
        "select map_values(map(\"maths\",90,\"science\",100));\n",
        "select map(\"maths\",90,\"science\",100)[\"science\"];\n",
        "select map(\"maths\",90,\"science\",100)[\"science1\"];\n",
        "select size(array(\"test\",\"test1\",\"test2\"));\n",
        "show tables;\n",
        "select replace(line,\"::\",\":\") from ratings_ip limit 10;\n",
        "dfs -ls /hive;\n",
        "insert overwrite directory \"/hive/movielens/ratings\"  select replace(line,\"::\",\":\") from ratings_ip ;\n",
        "dfs -cat /hive/movielens/ratings/*;\n",
        "dfs -ls  /hive/;\n",
        "dfs -ls  /hive/movielens;\n",
        "dfs -ls  /hive/movielens/ratings;\n",
        "select cast (956704519 as int)\n",
        ";\n",
        "select cast (956704519999999 as int)\n",
        ";\n",
        "select cast (9999999999999 as int)\n",
        ";\n",
        "select cast (9999999999999 as bigint);\n",
        "select cast (9999999999 as int);\n",
        "select cast (9999999 as int);\n",
        "select cast (99999999 as int);\n",
        "select cast (999999999 as int);\n",
        "select cast (9999999999 as int);\n",
        "create table hdpmovielens.ratings\n",
        "(userid int,\n",
        " movieid int ,\n",
        " rating int,\n",
        " timestamp bigint)\n",
        "row format delimited fields terminated by ':'\n",
        "stored as textfile\n",
        "location '/hive/movielens/ratings';\n",
        "create table hdpmovielens.ratings\n",
        "(userid int,\n",
        " movieid int ,\n",
        " rating int,\n",
        " time bigint)\n",
        "row format delimited fields terminated by ':'\n",
        "stored as textfile\n",
        "location '/hive/movielens/ratings';\n",
        "use hdpmovielens;\n",
        "select id from ratings limit 10;\n",
        "select userid from ratings limit 10;\n",
        "select ratings from ratings limit 10;\n",
        "select rating from ratings limit 10;\n",
        "select movieid,count(movieid) as cnt from ratings  group by movieid ;\n",
        "select movieid,count(movieid) as cnt from ratings where movieid=661  group by movieid ;\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QkHFrFyjhIkX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Ratings, users and movies::\n",
        "DATE:::    17 sept 2018\n",
        "     \n",
        "create table movie_datalake.users_ip( line string)\n",
        "stored as textfile;\n",
        "\n",
        "UserID::Gender::Age::Occupation::Zip-code\n",
        "load data local inpath '${system:user.home}/Movielens_wd_morning/ml-1m/users.dat' into table movie_datalake.users_ip;\n",
        "\n",
        "insert overwrite directory \"/hive/movielens/user\"  select replace(line,\"::\",\":\") from movie_datalake.users_ip ;\n",
        "create table hdpmovielens.users \n",
        "(userid int,\n",
        " gender string ,\n",
        " age int, \n",
        " occupation string,\n",
        "zipcode int)\n",
        "row format delimited fields terminated by ':'\n",
        "stored as textfile\n",
        "location '/hive/movielens/user';\n",
        "select * from hdpmovielens.users limit 10;\n",
        " \n",
        "  \n",
        "  MovieID::Title::Genres\n",
        "  create table movie_datalake.movies_ip( line string)\n",
        "stored as textfile;\n",
        "\n",
        "load data local inpath '${system:user.home}/Movielens_wd_morning/ml-1m/movies.dat' \n",
        "into table movie_datalake.movies_ip;\n",
        "\n",
        "insert overwrite directory \"/hive/movielens/movies\"  select replace(line,\"::\",\"@\") \n",
        "from movie_datalake.movies_ip ;\n",
        "\n",
        "\n",
        "  MovieID::Title::Genres\n",
        "movieid, title , release_year , genres (ARRAY)\n",
        "\n",
        "create table hdpmovielens.movies_stg\n",
        "(movieid int,\n",
        " title string ,\n",
        " genres Array<string>\n",
        ")\n",
        "row format delimited fields terminated by '@'\n",
        "collection items terminated by \"|\"\n",
        "stored as textfile\n",
        "location '/hive/movielens/movies';\n",
        "\n",
        "hive (movie_datalake)> select title from hdpmovielens.movies_stg;\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "select substr(\"3845::And God Created Woman (Et Dieu&#8230;Créa la Femme) (1956)::Drama\",60,4);\n",
        "select reverse(substr(reverse(\"3845::And God Created Woman (Et Dieu&#8230;Créa la Femme) (1956)::Drama\"),instr(reverse(\"3845::And God Created Woman (Et Dieu&#8230;Créa la Femme) (1956)::Drama\"),\")\")+1,4));\n",
        "\n",
        "\n",
        "select reverse(substr(reverse(\"3845::And God Created Woman (Et Dieu&#8230;Créa la Femme) (1956)::Drama\"),instr(reverse(\"3845::And God Created Woman (Et Dieu&#8230;Créa la Femme) (1956)::Drama\"),\")\")+1,4));\n",
        "\n",
        "\n",
        "select title from hdpmovielens.movies_stg;\n",
        "select reverse(substr(reverse(title),\n",
        "                      instr(reverse(title),\")\")+1,4)) from hdpmovielens.movies_stg;\n",
        "\n",
        "\n",
        "select reverse(substr(reverse(title),\n",
        "                      instr(reverse(title),\")\")+1,4)) from hdpmovielens.movies_stg;\n",
        "\n",
        "create table hdpmovielens.movies\n",
        "(movieid int,\n",
        " title string ,\n",
        " release_year int, \n",
        " genres Array<string>\n",
        ")\n",
        "row format delimited fields terminated by '@'\n",
        "collection items terminated by \"|\"\n",
        "stored as textfile;\n",
        "\n",
        "insert overwrite table hdpmovielens.movies select movieid,title,\n",
        "                      reverse(substr(reverse(title),\n",
        "                      instr(reverse(title),\")\")+1,4)) as title_year,\n",
        "                      genres from hdpmovielens.movies_stg ;\n",
        "\n",
        "select release_year, movieid from hdpmovielens.movies limit 10;\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_Vxm9KpkSW_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Date 18 Sept 2018 :::\n",
        "      \n",
        "      select name,col1 from student lateral view explode (array(name,dept,marks,gender)) x as col1 where x.col1=\"90\";\n",
        "\n",
        "      \n",
        " 1 . get Distinct genres:::\n",
        "      \n",
        "      select distinct  A.* from (select explode( genres) from movies) A  ;\n",
        "      hive (hdpmovielens)> select distinct  A.g1 from movies lateral view explode( genres) A as g1 ;\n",
        "      https://www.ericlin.me/2013/09/how-to-use-hive-lateral-view-in-your-query/\n",
        "        \n",
        "         select distinct  movies.title,A.g1 \n",
        "          from movies lateral view explode( genres) A as g1 where A.g1 rlike 'Children\\'s'\n",
        "\n",
        "           select distinct  movies.title,A.g1 from movies lateral view explode( genres) A as g1 where A.g1 like '%me%';\n",
        "select count(title)  from movies \n",
        "lateral view explode( genres) A as g1 where A.g1 like '%me%';\n",
        "1538\n",
        "\n",
        "select count(title),g1  \n",
        "from movies lateral view explode( genres) A as g1 where A.g1 like '%me%'\n",
        "group by A.g1;\n",
        "_c0     g1\n",
        "1200    Comedy\n",
        "211     Crime\n",
        "127     Documentary\n",
        "\n",
        "select title from movies where  array_contains(genres,\"Documentary\")\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HVnFcJk23tkz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "1. Top ten most viewed movies with their movies Name (Ascending or Descending order)\n",
        "ratings user id ,movie id rating when ?\n",
        "movie  mvid and title \n",
        "\n",
        "movieid group and count of each movie -> winoding  10 result jon data \n",
        "\n",
        "select count(1),movieid from ratings group by movieid;\n",
        "select count(*),movieid from ratings group by movieid;\n",
        "select count(movieid),movieid from ratings group by movieid;\n",
        "\n",
        "select r.movieid, count(r.movieid) cnt\n",
        "from ratings r\n",
        "group by r.movieid\n",
        "order by cnt desc\n",
        "limit 10;\n",
        "\n",
        "\n",
        "select r.movieid, count(r.movieid) cnt, m.title\n",
        "from ratings r join movies m ON r.movieid=m.movieid\n",
        "group by r.movieid,m.title\n",
        "order by cnt desc\n",
        "limit 10;\n",
        "\n",
        "select A.*,m.title\n",
        "from \n",
        "(select r.movieid, count(r.movieid) cnt\n",
        "from ratings r\n",
        "group by r.movieid\n",
        "order by cnt desc\n",
        "limit 10) A join movies m ON A.movieid=m.movieid\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cg9TDRY12m-A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Date: 19 Sept 2018\n",
        "   yarn application -list \n",
        "   set mapreduce.job.name;\n",
        "\n",
        "  dense_rank()\n",
        " \n",
        "\n",
        "select B.*,m.title from\n",
        "(select A.*, dense_rank() over (order by A.cnt desc)  as rnk\n",
        "from\n",
        "(select r.movieid, count(r.movieid) cnt\n",
        "from ratings r\n",
        "group by r.movieid) A )B \n",
        "join movies m ON B.movieid=m.movieid\n",
        "\n",
        "where B.rnk<=10;\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i2o3nkrO9efr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Analytic and window function\n",
        "stu1,dept1,90\n",
        "stu2,dept2,80\n",
        "stu3,dept1,60\n",
        "stu4,dept2,80\n",
        "stu5,dept1,90\n",
        "stu6,dept1,60\n",
        "stu7,dept2,70\n",
        "stu8,dept1,30\n",
        "stu9,dept2,40\n",
        "\n",
        "create table students (id string,dept string,marks int)\n",
        "row format delimited \n",
        "fields terminated by ','\n",
        "stored as textfile\n",
        "\n",
        "hdfs dfs -appendToFile - /user/hive/warehouse/hdpmovielens.db/students/stu.txt\n",
        "hdfs dfs  -cat  /user/hive/warehouse/hdpmovielens.db/students/stu.txt\n",
        "\n",
        "\n",
        "\n",
        "select *,\n",
        "rank() over (order by s.marks desc ) as rank,\n",
        "dense_rank() over (order by s.marks desc ) as drank,\n",
        "row_number() over (order by s.marks desc ) as rno,\n",
        "\n",
        "rank() over (partition by dept order by s.marks desc ) as dept_rank,\n",
        "dense_rank() over (partition by dept order by s.marks desc ) as dept_drank,\n",
        "row_number() over (partition by dept order by s.marks desc ) as dept_rno\n",
        "from students s\n",
        "\n",
        "\n",
        "select * from students\n",
        "where marks = (select max(marks) \n",
        "                from students \n",
        "                where marks <> (select max(marks) from students\n",
        "                )\n",
        "              );\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Add gender column to students table..\n",
        "\n",
        "\n",
        "stu1,dept1,90,M\n",
        "stu2,dept2,80,F\n",
        "stu3,dept1,60,M\n",
        "stu4,dept2,80,M\n",
        "stu5,dept1,90,M\n",
        "stu6,dept1,60,F\n",
        "stu7,dept2,70,M\n",
        "stu8,dept1,30,M\n",
        "stu9,dept2,40,M\n",
        "\n",
        "create table students (id string,dept string,marks int)\n",
        "row format delimited \n",
        "fields terminated by ','\n",
        "stored as textfile;\n",
        "\n",
        "truncate table students;\n",
        "alter table students add columns (gender string) ;\n",
        "\n",
        "dept   male female \n",
        "dept1   4   1\n",
        "dept2   4   1\n",
        "\n",
        "class M F\n",
        "      8 2\n",
        "  \n",
        "  select sum(if(gender='M',1,0)) male, \n",
        "  sum(if(gender='F',1,0)) female\n",
        "  from students;\n",
        "  \n",
        "  \n",
        "  select dept,sum(if(gender='M',1,0)) male, \n",
        "  sum(if(gender='F',1,0)) female\n",
        "  from students\n",
        "  group by dept;\n",
        "  \n",
        "  case column  \n",
        "  when A  then do something -> column=A \n",
        "  when B \n",
        "  else xyz\n",
        "  end\n",
        "  \n",
        "  \n",
        "  case \n",
        "  when column=A then \n",
        "  when column=B\n",
        "  else\n",
        "  end\n",
        "  \n",
        "  select sum(\n",
        "        \n",
        "     (case gender \n",
        "      when 'M' \n",
        "      then 1 \n",
        "      else 0 \n",
        "      end)\n",
        "  \n",
        "  ) male, \n",
        "  sum((case  \n",
        "       when gender='F' \n",
        "       then 1 \n",
        "       else 0 end)) female\n",
        "  from students;\n",
        "  \n",
        "  \n",
        "  hdfs dfs -appendToFile - /user/hive/warehouse/hdpmovielens.db/students/stu.txt\n",
        "\n",
        "  stu10,dept2,NULL,NULL\n",
        "\n",
        "  Question:\n",
        "    dept wise avg marks? \n",
        "    \n",
        "    select avg(marks), dept from students group by dept;\n",
        "  \n",
        "  hive (hdpmovielens)> select marks,NVL(marks,0) from students;\n",
        "hive (hdpmovielens)> select marks,NVL(marks,0) from students\n",
        "where marks is null;\n",
        "\n",
        "coalesce:\n",
        "  \n",
        "HOME WORK::\n",
        "    Windowning and analytic functions :: on MARKS column\n",
        "        SUM \n",
        "        AVG\n",
        "        COUNT\n",
        "        LEAD\n",
        "        LAG\n",
        "        FIRST\n",
        "        LAST\n",
        "    Solve remainig 3 question\n",
        "    \n",
        "    CUSTOM UDF- with examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AuaWdgUsJKAk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "20th Sept 2018\n",
        "\n",
        "hive (hdpmovielens)> select from_unixtime(unix_timestamp()),\n",
        "current_timestamp();\n",
        "\n",
        "\n",
        "\n",
        "yyyy-MM-dd HH:mm:ss.SSS\n",
        "    \n",
        "    \n",
        "    if the string is in standard format - convert into other format - \n",
        "    1) date_format(date/timestamp/string ts, string fmt)\n",
        "    \n",
        "   if dd-MM-yyyy into MM-dd-yyyy\n",
        "  unix_timestamp(string date, string pattern) into bigint\n",
        "  from_unixtime(bigint unixtime[, string format])\n",
        "  \n",
        "  \n",
        "  2019-01-31 convert this into 31-01-2019 and 01-31-2019\n",
        "  select \"2019-01-31\", \n",
        "  unix_timestamp(\"2019-01-31 00:00:00\"),\n",
        "  from_unixtime(unix_timestamp(\"2019-01-31 00:00:00\"),\"MM-dd-yyyy\"),\n",
        "  from_unixtime(unix_timestamp(\"2019-01-31 00:00:00\"),\"dd-MM-yyyy\"),\n",
        "  date_format(\"2019-01-31\",\"MM-dd-yyyy\"),\n",
        "  from_unixtime(unix_timestamp(\"2019-jan-31\",\"yyyy-MMM-dd\"),\"dd-MM-yyyy\");\n",
        "  \n",
        "  \n",
        "  \n",
        "  01-31-2019 -> convert this into standard format ::MM-dd-yyyy\n",
        "      \n",
        "      select from_unixtime(unix_timestamp(\"01-31-2019\",\"MM-dd-yyyy\"),\"dd-MMMM-yyyy\");\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4DUHIYulGbpd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "Cross\n",
        "dept stu1 stu2 stu3  s5 s6 7 8 9\n",
        "dept1  90  \n",
        "dept 2               80 90\n",
        "\n",
        "\n",
        "select dept, map(name,marks) from student where dept='dept1';\n",
        "\n",
        "select dept, collect_list(map(name,marks)) from student  group by dept;\n",
        "\n",
        "\n",
        "dept1   [{\"stu1\":90,\"stu3\":60,\"stu5\":90,\"stu6\":60},{\"stu8\":30}]\n",
        "dept2   [{\"stu2\":80},{\"stu4\":80},{\"stu7\":70},{\"stu9\":40}]\n",
        "\n",
        "hive (hdpmovielens)> select dept, collect_list(concat_ws(\"-\",name,cast(marks as string))) from student  group by dept;\n",
        "\n",
        "create table test as select dept, collect_list(concat_ws(\"-\",name,cast(marks as string))) l1 from student  group by dept;\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "select dept, collect_list(map(name,marks)) from student  group by dept;\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "select dept, explode(collect_list(map(name,marks))) from student  group by dept;\n",
        "\n",
        "drop table stg_students;\n",
        "create table stg_students as \n",
        "select dept, collect_list(map(name,marks)) as stu_list from student  group by dept;\n",
        "\n",
        "\n",
        "select dept,explode(stu_list) from stg_students;\n",
        "\n",
        "select dept,col1 from stg_students  lateral view explode (stu_list) V1 as col1 ;\n",
        "\n",
        "\n",
        "\n",
        "select dept, collect_list(map(name,marks))['stu1'] as \"stu1\" ,\n",
        "collect_list(map(name,marks))['stu2'] as \"stu2\" ,\n",
        "collect_list(map(name,marks))['stu3'] as \"stu3\" ,\n",
        "collect_list(map(name,marks))['stu4'] as \"stu4\" ,\n",
        "collect_list(map(name,marks))['stu5'] as \"stu5\" ,\n",
        "collect_list(map(name,marks))['stu6'] as \"stu6\" ,\n",
        "collect_list(map(name,marks))['stu7'] as \"stu7\" ,\n",
        "collect_list(map(name,marks))['stu8'] as \"stu8\" ,\n",
        "collect_list(map(name,marks))['stu9'] as \"stu9\" \n",
        "from student group by dept;\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-e5Zwim3MtTQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "3. Top twenty rated movies (which is calculated in the previous step) with no of views in the\n",
        "following age group\n",
        "(Age group : 1. Young (<20 years),\n",
        "2. Young Adult(20-40 years),\n",
        "3.adult (> 40years) )\n",
        "\n",
        "drop table q2_ansser;\n",
        "create table q2_ansser as \n",
        "select A.*,m.title from (\n",
        "select avg(rating) avg_rating,count(r.movieid) cnt, r.movieid , \n",
        "    rank() over (order by avg(rating) desc) rnk\n",
        "from ratings r \n",
        "group by r.movieid\n",
        "having count(r.movieid)>40\n",
        "    \n",
        ") A join movies m on A.movieid=m.movieid \n",
        "where A.rnk <=20\n",
        "\n",
        "hive (hdpmovielens)> select a.*,m.name from q2_ansser a join movies m on a.movieid=m.movieid;\n",
        "\n",
        "\n",
        "age group is present in user data\n",
        "mvid,userid in ratings\n",
        "moviid,name is in movies\n",
        "\n",
        "select * from user\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     20     40    60\n",
        "mvid count  count count\n",
        "----------------------------------\n",
        "\n",
        "convert age groups into 1,2,3 \n",
        "sum (case agegroup when 1 | 18 then 1 else 0) age_20\n",
        "sum (case agegroup when ,...| 18 then 1 else 0) age_40\n",
        "sum (case agegroup when 1 | 18 then 1 else 0) age_20\n",
        "\n",
        "group by movieid from you ratings where ratings.movie id in q2-anser\n",
        "\n",
        "moviid  count 20\n",
        "movie    count 40 \n",
        "\n",
        "convert these values in to cross tab ...\n",
        "\n",
        "\n",
        "select count(1) from ratings r where r.movieid=318;\n",
        "2227\n",
        "g20 , g40, g60 \n",
        "select u.userid,u.age from users u join ratings r on u.userid=r.userid\n",
        "\n",
        "where r.movieid=318;\n",
        "\n",
        "select r.movieid, count(1) cnt, sum(case when u.age=1 or u.age=18 then 1 else 0 end) g20,\n",
        "       sum(case when u.age=25 or u.age=35 then 1 else 0 end) g40,\n",
        "       sum(case when u.age=45 or u.age=50 or u.age=56 \n",
        "           then 1 else 0 end) g60\n",
        "    from users u join ratings r on u.userid=r.userid\n",
        "where r.movieid in (select movieid from q2_ansser)\n",
        "group by r.movieid \n",
        "\n",
        "\n",
        "select max(q.avg_rating),q.rnk ,r.movieid, count(1) cnt, sum(case when u.age=1 or u.age=18 then 1 else 0 end) g20,\n",
        "       sum(case when u.age=25 or u.age=35 then 1 else 0 end) g40,\n",
        "       sum(case when u.age=45 or u.age=50 or u.age=56 \n",
        "           then 1 else 0 end) g60, q.title\n",
        "    from users u join ratings r on u.userid=r.userid\n",
        "    join q2_ansser q on r.movieid=q.movieid\n",
        "-- where r.movieid in (select movieid from q2_ansser)\n",
        "group by r.movieid ,q.title;\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HZhr51cCJtyz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "AUTOMATION - run this for every 1 hour (cron tab )\n",
        "as well as on demand \n",
        "1) user_ip,ratings_ip,movies_ip\n",
        "\n",
        "cleansing steps \n",
        "\n",
        "q2_answer table \n",
        "\n",
        "final output \n",
        "\n",
        "prepare shell script to run all in sequence\n",
        "\n",
        "when each query completes - echo query cleaning data has comleted\n",
        "-------------\n",
        "when every step completes remove checkpoing file\n",
        "statgin create check point file\n",
        "when the current is already in progress you print a message -> progress current is in progress\n",
        "\n",
        "----------------------------\n",
        "source tables \n",
        "rating will be partitined key\n",
        "movieid will be bucketing -> sort \n",
        "----------------------------\n",
        "database names you are going to put in a .properties file\n",
        "use the file in shell script \n",
        "---------------------------------\n",
        "set the job names properly \n",
        "loaddata_date(yyyy_mm_dd_HH_mm_ss)_step1\n",
        "------------------\n",
        "\n",
        "RUN the script usiong hive and beeline \n",
        "----------------------------------\n",
        "UDF hi my name is bdt my email id xyz@cisco.com\n",
        "and my alternate mail id is test@yahoo.co.in \n",
        "\n",
        "hi my name is bdt my email id xyz@cisco.com\n",
        "and my alternate mail id is test@gmail.com\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ADD JAR /home/dorababugjntup/Saluatation_UDF.jar;\n",
        "CREATE TEMPORARY FUNCTION addSalute as 'com.bdt.Salutation';\n",
        "desc function addSalute;\n",
        "desc function  extended addSalute;\n",
        "show functions like '*Sal*;\n",
        "show functions like '*Sal*';\n",
        "select name, gender, addSalute(name) , addSalute(gender,name) from student;\n",
        "select name, gender, addSalute(name) , addsalute(gender,name) from student;\n",
        "select name, gender, addsalute(name) , addsalute(gender,name) from student;\n",
        "select name, gender, Addsalute(name) , Addsalute(gender,name) from student;\n",
        "ADD FILE /home/dorababugjntup/upper.py\n",
        ";\n",
        "select transform(name) using 'python upper.py' from student;\n",
        "select transform(name) using 'python upper.py' as (CAP_NAME)  from student;\n",
        "\n",
        "\n",
        "\n",
        "package com.bdt;\n",
        "\n",
        "import org.apache.hadoop.hive.ql.exec.Description;\n",
        "import org.apache.hadoop.hive.ql.exec.UDF;\n",
        "import org.apache.hadoop.hive.ql.udf.UDFType;\n",
        "import org.apache.hadoop.io.Text;\n",
        "\n",
        "@Description(\n",
        "\tname = \"Salute_add\", \n",
        "\tvalue = \"_FUNC_(gender, name) - Adds salutation\", \n",
        "\textended = \"This function will return salutation and name\"\n",
        ")\n",
        "@UDFType(deterministic = true, stateful = false)\n",
        "public class Salutation extends UDF {\n",
        "   public Text evaluate(String str) {\n",
        "      return str == null ? null : new Text(\"Mr. \"+str);\n",
        "   }\n",
        "   public Text evaluate(String gender,String name) {\n",
        "\t   \n",
        "\t   if(gender.equalsIgnoreCase(\"M\")){\n",
        "\t\t\tString str=\t\"Mr. \" + name;\n",
        "\t\t\treturn name == null ? null : new Text(str);\n",
        "\t   }else\n",
        "\t\t   return name == null ? null : new Text(\"Mrs. \"+ name);\n",
        "\t   \n",
        "      //return name == null ? null : new Text(str);\n",
        "   }\n",
        "   \n",
        "  /* public static void main(String[] args) {\n",
        "\t   Salutation obj=new Salutation();\n",
        "\t  System.out.println(obj.evaluate(\"BigDataTech\"));\n",
        "\t  System.out.println(obj.evaluate(\"M\",\"BigDataTech\"));\n",
        "\t  System.out.println(obj.evaluate(\"F\", \"BigDataTech\"));\n",
        "}*/\n",
        "   \n",
        "}\n",
        "\n",
        "\n",
        "---------------------------------\n",
        "\n",
        "#!/usr/bin/env python\n",
        "'''\n",
        "This is a script to upper all cases\n",
        "'''\n",
        "import sys\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        for line in sys.stdin:\n",
        "          n = line.strip()\n",
        "          print n.upper()\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":main()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kfWHb6Qcleho",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Unix framework to run the Hive queries:::\n",
        "Project:::: 24 sept 2018\n",
        "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "      job group -> RDBMS -> Ratings_Group  job_step_run is table name.\n",
        "pk jobgroupname jobstep description status created_date updated_date\n",
        "----------------------\n",
        "insert \n",
        "1 ratings_group 1 cleandata P now() now()\n",
        "\n",
        "1 ratings_group 1 cleandata R samedate now()\n",
        "\n",
        "1 ratings_group 1 cleandata E samedate now()\n",
        "\n",
        "1 ratings_group 1 cleandata C samedate now()\n",
        "---------------------------------------------\n",
        "\n",
        "write a unix function that takes status and step number\n",
        "print log -> updating status of step number as  ->status  at time\n",
        "----------------\n",
        "step1.hql\n",
        "step2.hql\n",
        "step3.hql\n",
        "-------------------\n",
        "write another function that runs the step\n",
        "fnciton (stepname)\n",
        "hive -f step1\n",
        "check the exit status\n",
        "update the status in RDBMs\n",
        "if success exit 0\n",
        "if failures update status as E in rdbms\n",
        "send a mail \n",
        "exit 1\n",
        "------------------------\n",
        "\n",
        "\n",
        "Begening check entries are present or not, if entries present print previous batch is pending\n",
        "run the pending steps.\n",
        "if no previous batch is present -> insert entreis in to control table and execute steps.\n",
        "\n",
        "At the end of job, check all jobs completed, then delete the entries .\n",
        "---------------------------\n",
        "All above is for control table.  job_step_run table \n",
        "\n",
        "\n",
        "job_step table ::::: \n",
        "Keep another table where each job how many steps are present? \n",
        "each step what is the job name and script name and location.\n",
        "shell script while executing extract the scipt name from these entries and run it using \n",
        "hive -f scrpt location.\n",
        "---------------------------\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h0GonLBkmHN6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ACID support in Hive :::: \n",
        "24 sept 2018"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e1k6buUW1gzB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SQOOP INSTLLATION\n",
        "https://dorababu-bigdata-trainer.blogspot.com/2018/09/sqoop-installation-in-gcp.html\n",
        "  \n",
        "  \n",
        "SQOOP download : wget http://www-us.apache.org/dist/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz\n",
        "    ls -lh sqoop*\n",
        "    tar -xvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz\n",
        "    mv sqoop-1.4.7.bin__hadoop-2.6.0 YARN/sqoop\n",
        "    \n",
        "    \n",
        "    vi $HOME/.bashrc\n",
        "export SQOOP_HOME=$HOME/YARN/sqoop\n",
        "export PATH=$PATH:$SQOOP_HOME/bin\n",
        "export SQOOP_CONF_DIR=$SQOOP_HOME/conf\n",
        "export SQOOP_CLASSPATH=$SQOOP_CONF_DIR\n",
        "\n",
        "source ~/.bashrc\n",
        "\n",
        "cp YARN/hive/lib/mysql-connector-java-8.0.11.jar YARN/sqoop/lib/\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nS4iUCuZ873T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sqoop list-databases \\\n",
        "--connect jdbc:mysql://localhost:3306/ \\\n",
        "--username root \\\n",
        "--password root \n",
        "      \n",
        "mysql -uroot -proot -e \"show databases\"\n",
        "\n",
        "sqoop eval \\\n",
        "--connect jdbc:mysql://localhost:3306/mysql \\\n",
        "--username root \\\n",
        "--password root \\\n",
        "      -e \"show tables\"\n",
        "      \n",
        "       mysql -uroot -proot -e \"use mysql;show tables\"\n",
        "        \n",
        "   sqoop list-tables \\\n",
        "--connect jdbc:mysql://localhost:3306/mysql \\\n",
        "--username root \\\n",
        "--password root \n",
        "        \n",
        "        \n",
        "        \n",
        "   Source data set::\n",
        "      wget https://github.com/dorababugjntup/SET2/raw/master/1_LoadMySQLData.tar.gz\n",
        "\n",
        "tar -xvf 1_LoadMySQLData.tar.gz\n",
        "cd 1_LoadMySQLData\n",
        "sh 1_LoadDataMySQL.sh\n",
        "user name: root  \n",
        "  password :: root\n",
        "   (y/n)small y\n",
        "  \n",
        "  \n",
        "  mysql -uroot -proot\n",
        "  show databases;\n",
        "  use complaints_db;\n",
        "  show tables;\n",
        "  mysql> select count(1) from complaints;\n",
        "mysql> select count(1) from usstates;\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SBgJFR-l935T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Date 26 sept 2018\n",
        "Sqoop import\n",
        "------------------------\n",
        "sqoop import \\\n",
        "--connect \"jdbc:mysql://localhost:3306/complaints_db?zeroDateTimeBehavior=CONVERT_TO_NULL\" \\\n",
        "--username root \\\n",
        "--password root \\\n",
        "--table usstates \\\n",
        "--enclosed-by '\"' \\\n",
        "--delete-target-dir \\\n",
        "--columns id,state \\\n",
        "-m 10 \\\n",
        "--target-dir /11111 \\\n",
        "--where \"state like 'A%'\" \n",
        "\n",
        "\n",
        "\n",
        "--boundary-query \"select 1,1000\" \\\n",
        "\n",
        "--split-by id \\\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sqoop import \\\n",
        "--connect \"jdbc:mysql://localhost:3306/complaints_db?zeroDateTimeBehavior=CONVERT_TO_NULL\" \\\n",
        "--username root \\\n",
        "--password root \\\n",
        "--enclosed-by '\"' \\\n",
        "--delete-target-dir \\\n",
        "--columns id,state \\\n",
        "-m 2\\\n",
        "--split-by id \\\n",
        "--target-dir /11111 \\\n",
        "-e \" select u.id,u.state from usstates u where state like 'A%' and \\$CONDITIONS \"\n",
        "\n",
        "\"select * from tableA ,table B where A.c1=B1.c2 and \\$CONDITIONS\n",
        "\n",
        "SELECT MIN(t1.id), MAX(t1.id) FROM ( select u.id,u.state from usstates u where state like 'A%' and  (1 = 1)  ) AS t1\n",
        "\n",
        "hive import\n",
        "incremental import\n",
        "sqoop job\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7e0kQkeMHTKb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Date 27 Sept 2018\n",
        "Create usstates_part table partitioned by start letter - column name is start_name\n",
        "------------------\n",
        "\n",
        "Via Sqoop create a table in hdpmovielens - same table name \n",
        "and peform full refresh\n",
        "--------------------------\n",
        "hdfs dfs -rm -r hdfs://localhost:9000/user/dorababugjntup/usstates \n",
        "    cp YARN/hive/lib/hive-exec-2.3.3.jar YARN/sqoop/lib/\n",
        "    cp YARN/hive/lib/mysql-con* YARN/sqoop/lib/\n",
        "sqoop import \\\n",
        "--connect \"jdbc:mysql://localhost:3306/complaints_db?zeroDateTimeBehavior=CONVERT_TO_NULL\" \\\n",
        "--username root \\\n",
        "--password root \\\n",
        "--hive-import \\\n",
        "--create-hive-table \\\n",
        "--hive-database hdpmovielens \\\n",
        "--table usstates \\\n",
        "--delete-target-dir \n",
        "\n",
        "\n",
        "hive (hdpmovielens)> select id,created_date,modified_date,state from usstates;\n",
        "\n",
        "in mysql::\n",
        "    insert into usstates (state,standardabbreviation,postalcode,capitalcity) values(\"aaa\",\"aaa\",\"aaa\",\"aaa\");\n",
        "\n",
        "export lastval=`hive -S -i test.hql -e \"select max(id) from hdpmovielens.usstates\"`\n",
        "\n",
        "sqoop import \\\n",
        "--connect \"jdbc:mysql://localhost:3306/complaints_db?zeroDateTimeBehavior=CONVERT_TO_NULL\" \\\n",
        "--username root \\\n",
        "--password root \\\n",
        "--hive-import \\\n",
        "--hive-database hdpmovielens \\\n",
        "--table usstates \\\n",
        "--hive-table usstates \\\n",
        "--incremental append \\\n",
        "--check-column id \\\n",
        "--last-value $lastval \n",
        "\n",
        "export lastval=`hive -e \"select max(id) from hdpmovielens.usstates\"`\n",
        "\n",
        "\n",
        "To store the last vlaue create a sqoop job\n",
        "\n",
        "sqoop job --list\n",
        "sqoop job --delete usstates_nonmutable\n",
        "\n",
        "sqoop job --create usstates_nonmutable \\\n",
        "-- import \\\n",
        "--connect \"jdbc:mysql://localhost:3306/complaints_db?zeroDateTimeBehavior=CONVERT_TO_NULL\" \\\n",
        "--username root \\\n",
        "--password root \\\n",
        "--table usstates \n",
        "--hive-import \\\n",
        "--hive-database hdpmovielens \\\n",
        "--table usstates \\\n",
        "--hive-table usstates \\\n",
        "--incremental append \\\n",
        "--check-column id \\\n",
        "--last-value 0 \\\n",
        "select * from usstates where id >lastvalue and id <= max(id)\n",
        "\n",
        "sqoop job --list\n",
        "sqoop job --show jobid\n",
        "sqoop job --exec jobid -- --last-value 0 --delete-target-dir --hive-table xyz\n",
        "\n",
        "--------------------\n",
        "\n",
        "Mutable Data:: \n",
        "    Old records also gets modified ::: \n",
        "          Insert / update \n",
        "          \n",
        "          \n",
        "            sqoop import \\\n",
        "  --connect \"jdbc:mysql://localhost:3306/complaints_db?zeroDateTimeBehavior=CONVERT_TO_NULL\" \\\n",
        "  --username root \\\n",
        "  --password root \\\n",
        "  --table usstates \\\n",
        "  --incremental lastmodified \\\n",
        "  --check-column modified_date \\\n",
        "  --target-dir /user/hive/warehouse/hdpmovielens.db/usstates \\\n",
        "  --fields-terminated-by '\\001' \\\n",
        "  --append \n",
        "\n",
        "           \n",
        " \n",
        "         \n",
        "            sqoop import \\\n",
        "  --connect \"jdbc:mysql://localhost:3306/complaints_db?zeroDateTimeBehavior=CONVERT_TO_NULL\" \\\n",
        "  --username root \\\n",
        "  --password root \\\n",
        "  --table usstates \\\n",
        "  --incremental lastmodified \\\n",
        "  --check-column modified_date \\\n",
        "  --target-dir /user/hive/warehouse/hdpmovielens.db/usstates \\\n",
        "  --fields-terminated-by '\\001' \\\n",
        "  --append \n",
        "    \n",
        "    \n",
        "    update usstates set state=\"ZZZZ\" where state like 'P%';\n",
        " insert into usstates (state,standardabbreviation,postalcode,capitalcity) values(\"bbb\",\"bbb\",\"bbb\",\"bbb\");\n",
        "\n",
        "    \n",
        "\n",
        "     create table ctrltbl (maxdate string) partitioned by (jobname string);\n",
        "            \n",
        "            set hive.exec.dynamic.partition.mode=nonstrict;\n",
        "            insert overwrite table ctrltbl partition(jobname)\n",
        "            select max(modified_date),\"usstates\" from usstates;\n",
        "            \n",
        "    \n",
        "    export lastval=`hive -S -i test.hql -e \"select  maxdate from hdpmovielens.ctrltbl\"`\n",
        "\n",
        "    \n",
        "    hdfs dfs -rm -r /user/hive/warehouse/hdpmovielens.db/usstates_incr\n",
        "    \n",
        "        sqoop import \\\n",
        "  --connect \"jdbc:mysql://localhost:3306/complaints_db?zeroDateTimeBehavior=CONVERT_TO_NULL\" \\\n",
        "  --username root \\\n",
        "  --password root \\\n",
        "  --table usstates \\\n",
        "  --incremental lastmodified \\\n",
        "  --check-column modified_date \\\n",
        "  --target-dir /user/hive/warehouse/hdpmovielens.db/usstates_incr \\\n",
        "  --fields-terminated-by '\\001' \\\n",
        "  --last-value \"$lastval\" \\\n",
        "      --append \n",
        "\n",
        "\n",
        "      CREATE TABLE `usstates_incr`(\n",
        "  `state` string,\n",
        "  `standardabbreviation` string,\n",
        "  `postalcode` string,\n",
        "  `capitalcity` string,\n",
        "  `created_date` string,\n",
        "  `modified_date` string,\n",
        "  `id` bigint)\n",
        "COMMENT 'Imported by sqoop on 2018/09/27 03:49:48'\n",
        "ROW FORMAT SERDE\n",
        "  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
        "WITH SERDEPROPERTIES (\n",
        "  'field.delim'='',\n",
        "  'line.delim'='\\n',\n",
        "  'serialization.format'='')\n",
        "STORED AS INPUTFORMAT\n",
        "  'org.apache.hadoop.mapred.TextInputFormat'\n",
        "OUTPUTFORMAT\n",
        "  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
        "LOCATION\n",
        "  'hdfs://localhost:9000/user/hive/warehouse/hdpmovielens.db/usstates_incr'\n",
        "TBLPROPERTIES (\n",
        "  'transient_lastDdlTime'='1538104179')\n",
        "\n",
        "\n",
        "set hive.exec.dynamic.partition.mode=nonstrict;\n",
        "            insert overwrite table ctrltbl partition(jobname)\n",
        "            select max(modified_date),\"usstates\" from usstates_incr;\n",
        "            \n",
        "we got old(ustates) and incr(usstate_incr)\n",
        "\n",
        "create table usstates_replic as select * from usstates;\n",
        "\n",
        "\n",
        "\n",
        "insert overwrite table usstates\n",
        "select B.* from usstates_incr B\n",
        "union all\n",
        "select A.* from usstates_replic A where A.id not in\n",
        "(select incr.id from usstates_incr incr);\n",
        "\n",
        "insert overwrite table usstates\n",
        "select D.state,\n",
        "D.standardabbreviation,\n",
        "D.postalcode,\n",
        "D.capitalcity,\n",
        "D.created_date,\n",
        "D.modified_date,\n",
        "D.id\n",
        "\n",
        "from (\n",
        "select \n",
        "    C.* , \n",
        "    row_number() over \n",
        "    (partition by id order by modified_date desc) as rno\n",
        "  from (\n",
        "  select B.* from usstates_incr B\n",
        "  union all\n",
        "  select A.* from usstates_replic A ) C\n",
        ") D where D.rno=1;\n",
        "\n",
        "\n",
        "\n",
        "insert overwrite table usstates_replic as select * from usstates;\n",
        "dfs -rm -r usstates_replic;\n",
        "dfs -cp usstates  usstates_replic;\n",
        "hive>msck repair table usstates_replic;\n",
        "-----------------------------\n",
        "\n",
        "1) original - replic table, INCR table \n",
        "2) control table to store max date from incr table\n",
        "\n",
        "3) full refresh and as soon as data refreshed in original, \n",
        "overwrite replic table\n",
        "----- INCR steps\n",
        "4) sqoop into incr table\n",
        "5) update control table\n",
        "6) Removal of duplicates - take repli and incr -> \n",
        "insert overwrite original\n",
        "(if the table is partitioned , remove diuplciates only from the partitions preset in \n",
        "replic)\n",
        "7) copy the data from original to replic\n",
        "-------------------\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YtiGey6WNL8g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "APACHE SPARK - 3rd Oct 2018\n",
        "---------------------------------\n",
        "\n",
        "5th Oct - RDD properties\n",
        "\n",
        "\n",
        "wget http://www-eu.apache.org/dist/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz\n",
        " tar -xvf spark-2.3.2-bin-hadoop2.7.tgz\n",
        "mv spark-2.3.2-bin-hadoop2.7 YARN/spark\n",
        "\n",
        "vi .bashrc\n",
        "export SPARK_HOME=$HOME/YARN/spark\n",
        "export PATH=$PATH:$SPARK_HOME/bin\n",
        "\n",
        "source .bahsrc \n",
        "----------------\n",
        "ls -l YARN/spark\n",
        "which spark-shell\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cEKO6rUvzOPv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SCALA PROGRAMMING\n",
        "Data type hierarchy\n",
        "Variable Types val var lazy val\n",
        "\n",
        "\n",
        "Type Inference - identify the data type of a variable from assignment value.\n",
        "Type Safety - we can assign a varaibel with the same type "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ao66skRj5mz3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "FUnctional programming:::\n",
        "      1) functions are firstclass citizer, they can exists any where in sourceode\n",
        "      with name , without a name , accept args, return some value \n",
        "      function is code block between { }\n",
        "      \n",
        "      2) functions are objects -> assign a function to a variable \n",
        "      3) functions can be passed to other functions/methods and/or returns functions\n",
        "      the methods/funcitons that accept or return a functon are called higher order functions/methods\n",
        "    \n",
        "    Call by name and call by value ? what is the difference::\n",
        "        \n",
        "        \n",
        "        {\n",
        "            val x=10\n",
        "            val y=100\n",
        "            println(\"sum of x and y is \"+(x+y))\n",
        "            x+y\n",
        "        }\n",
        "        \n",
        "        => transformation / rocket symbol\n",
        " val sum= (x:Int,y:Int ) => {\n",
        "            println(\"inside sum function ---sum of x and y is \"+(x+y))\n",
        "            x+y\n",
        "        }:Int\n",
        "        \n",
        "  sum: (Int, Int) => Int = <function2>\n",
        "\n",
        " def sum_method(x:Int,y:Int):Int={\n",
        "       println(\"inside sum-method---sum of x and y is \"+(x+y))\n",
        "            x+y\n",
        " }\n",
        "  sum_method: (x: Int, y: Int)Int\n",
        "-------------------------------------------\n",
        "Write sub function::\n",
        "    val sub=(x:Int,y:Int)=>{\n",
        "         println(\"inside sub function ---sub of x and y is \"+(x-y))\n",
        "            x-y\n",
        "    }\n",
        "        \n",
        "    sub: (Int, Int) => Int = <function2>\n",
        "-----------------------------\n",
        "Higher order method::::\n",
        "        \n",
        "        def operation(a:Int,b:Int,fn:(Int,Int)=>Int)={\n",
        "            println(\"inside opearatioin method\")\n",
        "            fn(a,b)           \n",
        "        }\n",
        "        \n",
        "        val operation_fn=(a:Int,b:Int,fn:(Int,Int)=>Int)=>{\n",
        "            println(\"inside opearatioin method\")\n",
        "            fn(a,b)           \n",
        "        }\n",
        "      \n",
        "      operation(21,20,(x,y)=>x%y)\n",
        "\n",
        "        \n",
        "         operation(10+20,sum_method(100,200),sum)\n",
        "          \n",
        "          operation(30,300,sum)-> 10+20 , call sum_methdo->go inside operation->\n",
        "          println inside operation -> \n",
        "          code for sum replaced-> inside fun -> sum(30,300)->result\n",
        "          \n",
        "          ==================================\n",
        "          call by value and call by name -> collection types in scala\n",
        "          tuple\n",
        "          array\n",
        "          range\n",
        "          list\n",
        "          map\n",
        "          string\n",
        "          diff b/n array and vector \n",
        "          \n",
        "              https://github.com/dorababugjntup/SET2/blob/master/SqFluOozHBase.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EWkmneOZ4VY0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Scala Collections - 9 Oct 2018\n",
        "import scala.collections.mutable._\n",
        " scala.collections.immutable is already imported.\n",
        "  \n",
        "  tuple ->  to pack multiple data types , it is not a collection -> no index/loops\n",
        "  scala> val t=(1,\"stu1\",200.50,Array(\"sub1\",\"sub2\"))\n",
        "\n",
        "  scala> val a2=Array(10,true)\n",
        "  \n",
        " val a= Array(10, 20, 30, 40)\n",
        " a(0)=10000\n",
        "  a.foreach((x:Int)=>{println(x)})\n",
        "a.foreach(x=>println(x))\n",
        "a.foreach(println)\n",
        " val stu=Array(\"stu1\",\"stu2\",\"stu3\")\n",
        "scala> val fn=(x:String)=> {\n",
        "     | val y=x+\"***\"\n",
        "     | println(y)\n",
        "     | y\n",
        "     | }\n",
        "fn: String => String = <function1>\n",
        "  scala> stu.map(fn)\n",
        "   stu.map(x=>{val y=x+\"***\";println(y);y})\n",
        "\n",
        " stu.map(x=>x+\"***\")\n",
        "stu.map(x=>x+\"***\").foreach(println)\n",
        "-------------------------------------------------------------------\n",
        "val t=(1,\"stu1\",200.50,Array(\"sub1\",\"sub2\"))\n",
        "t._1\n",
        "t._4\n",
        "t._4.(0)\n",
        "t._4(0)\n",
        "t\n",
        "t.toString\n",
        "t.toString.charAt(0)\n",
        "val t1=(10,\"stu1\")\n",
        "t._2\n",
        "val t=(1,\"stu1\",200.50,Array(\"sub1\",\"sub2\"))\n",
        "val t1=(10,\"stu10\")\n",
        "t1.swap\n",
        "t\n",
        "val s1=(t._2,t._3)\n",
        "val a=Array(10,20,30)\n",
        "val a1=Array(\"str1\",\"st2\")\n",
        "val a2=Array(10,true)\n",
        "val a2=Array(10,\"stu1\")\n",
        "val a2:Array[Any]=Array(10,true)\n",
        "val a2=Array(\"stu1\",List(10,20))\n",
        "val a2=Array(List(10,20),List(10,20))\n",
        "val a2=Array(List(10,true),List(10,20))\n",
        "val a2=Array(Array(10,true),List(10,20))\n",
        "val a2=Array(Array(10,true),List(10,20),Map(\"hi\"->\"stu1\")\n",
        ")\n",
        "t.productIterator\n",
        "for (a <- t.productIterator) println(a)\n",
        "for (a <- t.productIterator) println(a.getClass)\n",
        "t\n",
        "val a=Array(10,true,'c')\n",
        "val a=Array(10,true,\"c\")\n",
        "t.\n",
        "t\n",
        "a\n",
        "val a=Array(10,20,30,40)\n",
        "a(0)\n",
        "a\n",
        "a(3)\n",
        "a(4)\n",
        "a.length\n",
        "a\n",
        "a(0)=10000\n",
        "a\n",
        "a=Array(10,20)\n",
        "for( elem <- a ) println(elem)\n",
        "a.foreach((x:Int)=>{println(a)})\n",
        "a.foreach((x:Int)=>{println(x)})\n",
        "a.foreach(x=>println(x))\n",
        "a.foreach(println)\n",
        "val b=a.foreach(x=>println(x+10))\n",
        "b\n",
        "val stu=Array(\"stu1\",\"stu2\",\"stu3\")\n",
        "stu.foreach(println)\n",
        "a\n",
        "val b=a.map(x=>x=10)\n",
        "val b=a.map(x=>x+10)\n",
        "b\n",
        "val b=a.map(x=>x+\"hi\")\n",
        "val b=a.foreach(x=>x+\"hi\")\n",
        "stu\n",
        "stu.map(x=>10)\n",
        "stu.map(x=>{println(x)})\n",
        "stu.map(x=>{val y=x+\"***\";println(y);y})\n",
        "vla fn=(x:String)=> {\n",
        "val y=x+\"***\"\n",
        "println(y)\n",
        "y\n",
        "}\n",
        "val fn=(x:String)=> {\n",
        "val y=x+\"***\"\n",
        "println(y)\n",
        "y\n",
        "}\n",
        "stu.map(fn)\n",
        " stu.map(x=>{val y=x+\"***\";println(y);y})\n",
        "stu.map(x=>x+\"***\")\n",
        "stu.map(x=>x+\"***\").foreach(println)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ei77XzB59aWb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RANGE::\n",
        "    val r= 1 to 10 \n",
        " MAP val m=Map(\"maths\" -> 90 , \"science\" -> 80)\n",
        "TUPLE\n",
        "ARRAY\n",
        "LIST::: \n",
        "      (1 to 10 ).toList\n",
        "      List(1,2,3)\n",
        "      1::2::3::Nil\n",
        "                  \n",
        "   Create a list with every element having id,name, fee, gender\n",
        "  with 5 students\n",
        "  \n",
        "  val stu=List((3,\"stu3\",500.40,\"F\"),(1,\"stu1\",100.40,\"M\"),(5,\"stu5\",700.40,\"M\"),\n",
        "               (2,\"stu2\",200.40,\"F\"),\n",
        "               (4,\"stu4\",100.40,\"M\"))\n",
        "      \n",
        "  Operators:: \n",
        "      => transformation or rocket to define function\n",
        "      -> map key value separator\n",
        "      <- generator \n",
        "      . object.method\n",
        "      :: append \n",
        "      : -> variable:data type\n",
        "          \n",
        "      https://www.scala-lang.org/api/2.12.3/scala/collection/immutable/List.html\n",
        "      \n",
        "      \n",
        "      \n",
        "      623  stu\n",
        "624  l.foreach(println)\n",
        "625  stu\n",
        "626  stu.foreach(x=> println(x._2))\n",
        "627  (3,stu3,500.4,F)._2\n",
        "628  (3,\"stu3\",500.4,\"F\")._2\n",
        "629  stu.map(x=>(x._1,x._3))\n",
        "630  stu\n",
        "631  l\n",
        "632  l.map(_ + 10)\n",
        "633  l.map(x =>x+ 10)\n",
        "634  l.map(x =>x+ \"10)\"\n",
        "635  l.map(x =>x+ \"10\")\n",
        "636  l.map(x =>(x,x+10))\n",
        "637  l.map(x=> if(x%2==0) x)\n",
        "638  l.map(x=> if(x%2==0) x).size\n",
        "639  l.filter(x=> x%2==0 )\n",
        "640  l.filter(x=> x%2!=0 )\n",
        "641  l.filterNot(x=> x%2==0 )\n",
        "\n",
        "           \n",
        "           \n",
        "   647  import scala.collection.mutable.ListBuffer\n",
        "648  val l1=ListBuffer(1,2,3,4,5)\n",
        "649  l1.drop(2)\n",
        "650  l1 += 1000\n",
        "651  l1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RjlrXLa9ENEK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "11-Oct-2018::\n",
        "    foreach - it takes a function with one arg and return unit\n",
        "     x => UNIT \n",
        "    map -> return a collection by applying a funciton on source collection\n",
        "  no of elements and collectionm type will be same, \n",
        "  element data type can change\n",
        "  filter -> x => boolean \n",
        "  \n",
        "  val l=(1 to 10).toList\n",
        "  val stu=List((3,\"stu3\",500.40,\"F\"),(1,\"stu1\",100.40,\"M\"),(5,\"stu5\",700.40,\"M\"),\n",
        "               (2,\"stu2\",200.40,\"F\"),\n",
        "               (4,\"stu4\",100.40,\"M\"))\n",
        "  \n",
        "   val ml=stu.filter(x=>x._4.equalsIgnoreCase(\"m\"))\n",
        "     val fl=stu.filterNot(x=>x._4.equalsIgnoreCase(\"m\"))\n",
        "    val (ml,fl)=stu.partition(x=>x._4.equalsIgnoreCase(\"m\"))\n",
        "     l.partition(x=>x%2==0)._1 \n",
        "\n",
        "Question::       \"print name,gender of male students with fee < 500\"\n",
        "\n",
        "      \n",
        "      scala> stu.filter(x=>x._4.equalsIgnoreCase(\"m\")).filter(x=>x._3 < 500).map(t=>(t._2,t._4))\n",
        "res45: List[(String, String)] = List((stu1,M), (stu4,M))\n",
        "\n",
        "scala> stu.filter(x=>x._4.equalsIgnoreCase(\"m\") && x._3 < 500).map(t=>(t._2,t._4))\n",
        "res46: List[(String, String)] = List((stu1,M), (stu4,M))\n",
        "  \n",
        "  stu.\n",
        "  filter(x=>x._4.equalsIgnoreCase(\"m\")).\n",
        "  filter(x=>x._3 < 500).\n",
        "  map(t=>(t._2,t._4)).\n",
        "  foreach(println)\n",
        "\n",
        "  ------------\n",
        "  sort student list based on fee paid::\n",
        "      scala> stu.sortBy(t=>t._3).foreach(println)\n",
        "(1,stu1,100.4,M)\n",
        "(4,stu4,100.4,M)\n",
        "(2,stu2,200.4,F)\n",
        "(3,stu3,500.4,F)\n",
        "(5,stu5,700.4,M)\n",
        "\n",
        "scala> val l1=List(100,30,4,2,80)\n",
        "l1: List[Int] = List(100, 30, 4, 2, 80)\n",
        "\n",
        "scala> l1.sortBy(x=>x)\n",
        "res62: List[Int] = List(2, 4, 30, 80, 100)\n",
        "\n",
        "  Sort in reverse based on fee :: \n",
        "      stu.sortBy(t=>t._3).reverse.foreach(println)\n",
        "\n",
        " stu.sortBy(t=>t._3)(scala.math.Ordering[Double].reverse).foreach(println)\n",
        "(5,stu5,700.4,M) => x._4\n",
        "(3,stu3,500.4,F)\n",
        "(2,stu2,200.4,F)\n",
        "(1,stu1,100.4,M)\n",
        "(4,stu4,100.4,M)\n",
        "\n",
        "l1.sortWith((x,y)=>{println(x+\":::\"+y);x>y})\n",
        "scala> stu.sortWith((t1,t2)=> t1._3>t2._3).foreach(println)\n",
        "scala> stu.sortWith((t1,t2)=> t1._3<t2._3).foreach(println)\n",
        "scala> stu.reduceLeft((t1,t2)=>(0,\"\",t1._3+t2._3,\"\") )._3\n",
        "\n",
        "scala> l1.scanRight(0)((x,y)=>{println(x+\":::\"+y);x+y})\n",
        "\n",
        "scala> l1.foldLeft(0)((x,y)=>{println(x+\":::\"+y);x+y})\n",
        "scala> l1.reduceLeft((x,y)=>{println(x+\":::\"+y);x+y})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EgjuCSCrd7wA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scala - fiels rdbms\n",
        "spark files rdbms\n",
        "spark rdds data frames\n",
        "spark-sumit\n",
        "spark sql\n",
        "spark+hive\n",
        "flume+kafkak+streaming\n",
        "hbase\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EjLRZB4DR-R2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scala> l1.zipWithIndex\n",
        "scala> l1.zipWithIndex.unzip\n",
        "scala> val l2=(1 to 4).toList\n",
        "scala> l1.zip(l2)\n",
        "\n",
        "scala> l2.zipAll(l1,1000,\"$\")\n",
        "scala> l2.zipAll(l1,1000,\"$\")\n",
        "l1.mkString(\"start\",end=\"end\",sep=\"$\")\n",
        "\n",
        "union\n",
        "scala> \"hi welcome\".indexOf(\"w\")\n",
        "\n",
        "\n",
        "val s1=List(\"hi welcome to class\",\"spark and scala\")\n",
        "s1.map(line=>line.split(\" \"))\n",
        " s1.map(line=>line.split(\" \")).flatten\n",
        "s1.flatMap(line=>line.split(\" \")).foreach(println)\n",
        "\n",
        "word count:::\n",
        "      val s1=List(\"hi welcome to class\",\"spark and scala\",\"spark and scala\")\n",
        "      s1.flatMap(line=>line.split(\" \")).groupBy(x=>x)\n",
        "      Map(key,vlaue) -> word , list(words) -> map x(hi,list(hi,hi,hi))=> x._1,x._2.size\n",
        "      s1.flatMap(line=>line.split(\" \")).groupBy(x=>x).map(t=>(t._1,t._2.size))\n",
        "      \n",
        "      \n",
        "      val stu=List((3,\"stu3\",500.40,\"F\"),(1,\"stu1\",100.40,\"M\"),(5,\"stu5\",700.40,\"M\"),\n",
        "               (2,\"stu2\",200.40,\"F\"),\n",
        "               (4,\"stu4\",100.40,\"M\"))\n",
        "  \n",
        "Display M,F student count\n",
        "stu.groupBy(t=>t._4).map(t=>(t._1,t._2.size)).foreach(println)\n",
        "check how many M studetns present\n",
        "stu.count(t=>t._4==\"M\")\n",
        "get distinct values from collection\n",
        "stu.map(t=>t._4).distinct\n",
        "stu.map(t=>t._4).toSet\n",
        "\n",
        "Q) instead of group by use distinct and count \n",
        "scala> for(a<-stu.map(t=>t._4).distinct ) println(a+\":::\"+stu.count(t=>t._4==a))\n",
        "\n",
        "scala> for(a<-stu.map(t=>t._4).distinct ) yield(a,stu.count(t=>t._4==a))\n",
        "for(a<-stu.map(t=>t._4).distinct ) yield(a->stu.count(t=>t._4==a))\n",
        "\n",
        "import scala.collection.mutable.StringBuilder\n",
        "val sb=new StringBuilder(\"test\")\n",
        " l.addString(sb)\n",
        "\n",
        "  \n",
        "  ------------------------\n",
        "  Read data from File\n",
        "  \n",
        "1171  import scala.io.Source\n",
        "scala> for(line <- Source.fromFile(\"empdata\").getLines) println(line)\n",
        "\n",
        "1154  Source.fromFile(\"test\").mkString.split(\";\")(1)\n",
        "1155  Source.fromFile(\"test\").mkString.split(\";\")(3)\n",
        "1160  Source.fromFile(\"test\").toList   // charcters into list\n",
        "1162  Source.fromFile(\"test\").mkString\n",
        "1163  Source.fromFile(\"test\").getLines\n",
        "1164  Source.fromFile(\"test\").getLines.foreach(println)\n",
        "\n",
        "val lines = Source.fromFile(\"/Users/Al/.bash_profile\").getLines.toList  // all lines into List\n",
        "val lines = Source.fromFile(\"/Users/Al/.bash_profile\").getLines.toArray\n",
        "\n",
        "\n",
        "\n",
        "https://alvinalexander.com/scala/scala-jdbc-connection-mysql-sql-select-example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aEjr-R0avx93",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SPARK RDDS::\n",
        "    \n",
        "    We need collections(distributed) to hold data.\n",
        "    Spark -can be considered as  separate language -> have its own data type\n",
        "    Collectins to hold data in spark are::\n",
        "        RDD(unstructed data, created using SparkContext)\n",
        "        DF/DS( to hold structured data , created using SparkSession)\n",
        "        \n",
        "    \n",
        "    spark-shell --help\n",
        "    --master local[*] take all cores of cpu and create driver, driver itself acts as exectutor\n",
        "    --deploy-mode client\n",
        "    --conf PROP=VALUE --conf PROP=VALUE  --conf PROP=VALUE \n",
        "    --properties-file FILE \n",
        "    --driver-memory MEM\n",
        "    --driver-java-options\n",
        "     --executor-memory MEM \n",
        "      --driver-cores NUM\n",
        "      YARN-only:\n",
        "  --queue QUEUE_NAME \n",
        "  --num-executors NUM\n",
        "  \n",
        "  \n",
        "  When Spark-shell is running, it exposes services by WEB UI with externalIpAddress:4040\n",
        "  enable firewall rules to access this UI.\n",
        "  (vpc network->filrewall rules->create firewall rule->0.0.0.0/0 and protocols tcp:4040,4041,4042)\n",
        "  \n",
        "  Transformations:\n",
        "  RDD.map , filter, flatMap, -> PairRDD if every element is k-v pair RDD(key,value) \n",
        "  pair RDD have extra methods -> Joins are available only in pairRDD, key based operations\n",
        "  Actions\n",
        "  RDD.first,count,saveAsTextfile, \n",
        "  cache vs persist\n",
        "  \n",
        "  \n",
        "  \n",
        "  in Spark UI::\n",
        "      JOB -> Stage-> tasks\n",
        "      SHuffle read/shuffle Write"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q7maiGNs2ZXF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "16Oct2018\n",
        "Create RDD -> load from external data sets (textFile,Sequencefiles, hadoop files , object)\n",
        "or parallelize in memory sequence. or apply a transformatiin on RDD\n",
        "\n",
        "Create DF/ or DS -> load from stuctrured sources, RDBMS, orc,parquet\n",
        "or convert RDD by providing schema \n",
        "or apply sql in any language , output is DF\n",
        "or apply DF operations(DSL) creates DF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bzeaFZ9J6i9W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "1513  inputRDD.cache\n",
        "1515  inputRDD.persist( org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK_SER)\n",
        "1516  inputRDD.getStorageLevel\n",
        "1517  inputRDD.unpersist -> fials\n",
        "1518  inputRDD.unpersist(true)\n",
        "1519  inputRDD.getStorageLevel\n",
        "1520  inputRDD.persist( org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK_SER)\n",
        "1521  inputRDD.getStorageLevel\n",
        "1522  inputRDD.unpersist(true)\n",
        "1523  inputRDD.getStorageLevel\n",
        "-----------------\n",
        "1526  inputRDD.checkpoint\n",
        "1527  sc.getCheckpointDir\n",
        "1528  sc.setCheckpointDir(\"OutFiles/chkpointdir\")\n",
        "1529  sc.getCheckpointDir\n",
        "1530  inputRDD.checkpoint\n",
        "1531  inputRDD.count\n",
        "r1.getCheckpointFile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TYfgfh3uCAI2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "val inputRDD=sc.textFile(\"wc_input.txt\")\n",
        "val r1=inputRDD.flatMap(line=>line.split(\" \")).groupBy(x=>x).map(t=> (t._1,t._2.size))\n",
        "r1.saveAsTextFile(\"OutFiles/wc_input\")\n",
        "\n",
        "val inputRDD1=sc.textFile(\"wc_input.txt\")\n",
        "scala> inputRDD1.flatMap(line=>line.split(\" \")).groupBy(k=>k).map(t=>(t._1,t._2.size)).foreach(println)\n",
        "scala> inputRDD1.flatMap(line=>line.split(\" \")).map(word=>(word,1)).reduceByKey((x,y)=>x+y).foreach(println)\n",
        "\n",
        "inputRDD1.\n",
        "flatMap(line=>line.split(\" \")).   // convert line into words and flatte\n",
        "map(word=>(word,1)).              // convert RDD into PairRDD\n",
        "reduceByKey((x,y)=>x+y).          // take two values for a key and apply sum and repeat until there no aggregtation possibe;\n",
        "foreach(println)                  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g5Sx0KLgJkZL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "emp\n",
        "1,emp1,2000,dept1\n",
        "2,emp2,6000,dept2\n",
        "3,emp3,500,dept1\n",
        "4,emp4,7000,dept2\n",
        "5,emp5,10000,dept3\n",
        "6,emp6,4000,dept3\n",
        "\n",
        "dept\n",
        "dept1,hive\n",
        "dept2,oozie\n",
        "dept4,spark\n",
        "\n",
        "1) display all emp record, dept name \n",
        "val e=sc.textFile(\"emp\")\n",
        "val d=sc.textFile(\"dept\")\n",
        "val e_pair=e.map(line=>(line.split(\",\")(3),line))\n",
        "val d_pair=d.map(line=>(line.split(\",\")(0),line.split(\",\")(1)))\n",
        "e_pair.join(d_pair).foreach(t=>println(t._2))\n",
        "d_pair.join(e_pair).foreach(println)\n",
        "\n",
        "e_pair.leftOuterJoin(d_pair)\n",
        "e_pair.leftOuterJoin(d_pair).foreach(line=>println(line._2._1+\",\"+line._2._2.getOrElse(\"NotFound\")))\n",
        "\n",
        "join - dept, (all records for emp, all records for dept)\n",
        "full outer join  dept(option, option)\n",
        "left outer dept, emp, option (dept)\n",
        "right outer dept, ption()emp, (dept)\n",
        "\n",
        "https://github.com/dorababugjntup/Assignments/tree/master/3_MovieLens\n",
        "  \n",
        "  1)Download scala plugin in eclipse market place\n",
        "  2) Download spark in windows\n",
        "  \n",
        "  download SBT in windows and install\n",
        "  sbt plugin eclipse (sbt project folder structure)\n",
        "  \n",
        "  download git in windows\n",
        "  \n",
        "  practice maven java (hellow world ) in eclipse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vD_aQZ6D8uAj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HJ6NRQEbH702",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BroadCast Variables::\n",
        "    1771  val pws = Map(\"Apache Spark\" -> \"http://spark.apache.org/\", \"Scala\" -> \"http://www.scala-lang.org/\")\n",
        "1772  val searchWords=sc.parallelize(Seq(\"Apache Spark\",\"Scala\" ))\n",
        "1773  searchWords.map(x=>pws(x)).collect\n",
        "1774  searchWords.map(pws).collect\n",
        "1779  val pwsB=sc.broadcast(pws)\n",
        "1780  searchWords.map(pwsB.value).collect\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "val e=sc.textFile(\"emp\")\n",
        "val d_pair=scala.io.Source.fromFile(\"dept\").getLines.map(line => (line.split(\",\")(0),line.split(\",\")(1))).toMap\n",
        "val e_pair=e.map(line=>(line.split(\",\")(3),line))\n",
        "//val d_pair=d.map(line=>(line.split(\",\")(0),line.split(\",\")(1)))\n",
        "e_pair.map(t=>(t._2,d_pair.getOrElse(t._1,\"Not Found\"))).foreach(println)\n",
        "\n",
        "val e_pair=e.map(line=>(line,d_pair.getOrElse(line.split(\",\")(3),\"Not Found\")))\n",
        "\n",
        "\n",
        "val d_pair_B=sc.broadcast(d_pair)\n",
        "e_pair.map(t=>(t._2,d_pair_B.value.getOrElse(t._1,\"Not Found\"))).foreach(println)\n",
        "-----------------------------------------\n",
        "val e=sc.textFile(\"emp\")\n",
        "val d_pair=scala.io.Source.fromFile(\"dept\").getLines.map(line => (line.split(\",\")(0),line.split(\",\")(1))).toMap\n",
        "val e_pair=e.map(line=>(line,d_pair.getOrElse(line.split(\",\")(3),\"Not Found\")))\n",
        "e_pair.foreach(println)\n",
        "  \n",
        "  \n",
        "Map(dept1 -> hive, dept2 -> oozie, dept4 -> spark)\n",
        "  (5,emp5,10000,dept3,Not Found)\n",
        "(1,emp1,2000,dept1,hive)\n",
        "(6,emp6,4000,dept3,Not Found)\n",
        "(2,emp2,6000,dept2,oozie)\n",
        "(3,emp3,500,dept1,hive)\n",
        "(4,emp4,7000,dept2,oozie)  \n",
        "  \n",
        "  \n",
        "  emp left join dept -\n",
        "  val d_pair_B=sc.broadcast(d_pair)\n",
        "val e_pair=e.map(line=>(line,d_pair_B.value.getOrElse(line.split(\",\")(3),\"Not Found\")))\n",
        "e_pair.foreach(println)\n",
        "-----------------------------------------\n",
        "\n",
        "NOTE:::: We cannot Borad Cast  RDD, we can braodcast Data Frame and other non distributed collections\n",
        "        Advantages::: reduces n/w traffic and can perform mapside joins\n",
        "        \n",
        "        \n",
        "        without BV -> When ever an executor is started , the value will be sent through n/w to that node -increase n/w traffic\n",
        "with BV -> the variable is already sent once to the node , whenever new executors starts, no need to transfer the data,\n",
        "as it is already present in every node.\n",
        "Map side joins ( send second data set to every node and each task can use this data set for join)\n",
        "\n",
        "\n",
        "        \n",
        "        ---------------------\n",
        "        Accumulator:::\n",
        "              They can be used to implement counters (as in MapReduce) or sums. \n",
        "              it acts like a global variable\n",
        "              they are shared variables and value can be changed\n",
        "              val x=sc.longAccumulator(\"noOfLines\")\n",
        "              val e=sc.textFile(\"emp\")\n",
        "\n",
        "              e.foreach(line=>{x.add(10);println(line+\"***\"+x.count)})\n",
        "              \n",
        "              We can see the accumulator in Tasks of Stages page.\n",
        "              This varibale is alive till the session is active.\n",
        "              \n",
        "              To find sum of array\n",
        "              sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum.add(x))\n",
        "              accum.value gives 10.\n",
        "              \n",
        "Spark---------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d1Y2RsQnUKJ3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "SPARK DATA FRAME / DATA SET -> TABLE - we can apply RDD methods, DF/DS methods(DSL), SQL \n",
        "its an abstraction (collection in spark)to hold structured data.\n",
        "\n",
        "1.x -> DF -> RDD + SCHEMA \n",
        "How to create data frames??\n",
        "1) convert exising RDD , and provide schema rdd.toDF(schema)\n",
        "2) load data from variety of sources\n",
        "csv,json,RDBS,orc,parque, hive table, textfile\n",
        "-> 1.x to read csv we need to use external libraries databricks apis\n",
        "2.x all common formats to read, no need to external libraries\n",
        "\"SparkSession.read\" API\n",
        "3) any language , if you run sql ( DF=sparkSession.sql(query)), the output is Data frame\n",
        "4) apply any transformation (rdd,df dsl) that creates another DF\n",
        "5) convert DS into DF\n",
        "\n",
        "What is the difference between DS and DF\n",
        "http://spark.apache.org/docs/latest/sql-programming-guide.html\n",
        "\n",
        "val e=sc.textFile(\"emp\")\n",
        "val d=sc.textFile(\"dept\")\n",
        "val e_pair=e.map(line=>(line.split(\",\")(3),line))\n",
        "val d_pair=d.map(line=>(line.split(\",\")(0),line.split(\",\")(1)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oZ7ShQgjQgKp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Date 18th Oct 2018\n",
        "convert RDD into DF:::\n",
        "      val fn=(line:String)=>{\n",
        "          val arr=line.split(\",\")\n",
        "          (arr(0).toInt,arr(1),arr(2).toInt,arr(3))\n",
        "      }\n",
        "      fn(\"1,emp1,2000,dept1\")\n",
        "val empRDD=sc.textFile(\"emp\").map(fn)\n",
        "val empDF=empRDD.toDF(\"id\",\"name\",\"sal\",\"deptid\")\n",
        "  \n",
        "  \n",
        "  empDF -> org.apache.spark.sql.DataFrame - We dont have DataFrame api in scala,\n",
        "  DF is Dataset[Row], \n",
        "  DS is https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\n",
        "    Dataset is strongly typed - type checking at compile time\n",
        "    DataFrame is untyped view that is present in Dataset, type checking is at run time\n",
        "    \n",
        "  How to Create Dataset??? \n",
        "  RDD(jvmObjects)\n",
        "  \n",
        "  \n",
        "  case class Emp(id:Int,name:String,sal:Int,deptid:String)\n",
        "  val fn1=(line:String)=>{\n",
        "          val arr=line.split(\",\")\n",
        "          Emp(arr(0).toInt,arr(1),arr(2).toInt,arr(3)) // setting the values for Emp Object\n",
        "      }\n",
        "      fn1(\"1,emp1,2000,dept1\") -> Emp jvm object\n",
        "    \n",
        " val empRDD1=sc.textFile(\"emp\").map(fn1)  -> RDD of JVM objects (Strongly typed )\n",
        " val empDS=empRDD1.toDS()\n",
        "\n",
        "for toDF-> we need to supply column names - def toDF(colNames: String*): org.apache.spark.sql.DataFrame \n",
        "for toDS-> no args - def toDS(): org.apache.spark.sql.Dataset\n",
        "  first create RDD(jvmObjects) then convert using toDS\n",
        "\n",
        "  --------------------------------------------------\n",
        "  Create your own schama and load data using this schema\n",
        "  import org.apache.spark.sql._\n",
        "import org.apache.spark.sql.types._\n",
        "\n",
        "val struct = StructType(\n",
        "Seq(StructField(\"id\",IntegerType,false),\n",
        "StructField(\"name\",StringType,true), \n",
        "StructField(\"sal\",IntegerType,false), \n",
        "StructField(\"deptid\",StringType,true))\n",
        ") \n",
        "  \n",
        "  \n",
        "  \n",
        "  import org.apache.spark.sql.Row\n",
        "  val fn2=(line:String)=>{\n",
        "          val arr=line.split(\",\")\n",
        "          Row(arr(0).toInt,arr(1),arr(2).toInt,arr(3)) // setting the values for Emp Object\n",
        "      }\n",
        "      fn2(\"1,emp1,2000,dept1\")\n",
        "    res9: org.apache.spark.sql.Row = [1,emp1,2000,dept1]\n",
        "val empRDD2=sc.textFile(\"emp\").map(fn2)   // Create RDD of Row objects and privde your own schema\n",
        " val empDF2=spark.createDataFrame(empRDD2,struct)\n",
        " ---------------------------------------------------\n",
        "\n",
        "Summary::: create DF/DS from RDD(unstructured data)\n",
        "1) RDD.toDF(string column names) ->DF  -> not strictly typed\n",
        "RDD(jvmobject).toDS() -> first create RDD with jVM objects -> Strict Type checking\n",
        "(case class , every row of RDD is the object of case class)\n",
        "\n",
        "2)createStructType,\n",
        "RDD(row objects) and use spark.createDataFrame(RDD(row),structType)\n",
        "RDD(jvmobject) and use spark.createDataset(RDD(jvmobjects))\n",
        "spark.createDataset(empRDD1) same as empRDD1.toDS()\n",
        "-------------------------------------------------\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "btYTMejbi5-Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DataSet Operations::: (DSL -> domain specifig language)\n",
        "      1) Actions\n",
        "      2) Typed Transformations - return Dataset\n",
        "      3) UnTyped Transformatios - return DataFrame\n",
        "      4)Basic Dataset functions "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DBylSyX0jnA6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Demo of DF operations:: Create data frame::\n",
        "       val fn=(line:String)=>{\n",
        "          val arr=line.split(\",\")\n",
        "          (arr(0).toInt,arr(1),arr(2).toInt,arr(3))\n",
        "      }\n",
        "      fn(\"1,emp1,2000,dept1\")\n",
        "val empRDD=sc.textFile(\"emp\").map(fn)\n",
        "val empDF=empRDD.toDF(\"id\",\"name\",\"sal\",\"deptid\")\n",
        "#### use inmemory metastore\n",
        "#### If we save the DF as table -> uses derby metastore\n",
        "#### if we integrate with Hive metastore, we can access same tables in hive and spark.\n",
        "\n",
        "ACTIONS::\n",
        "    empDF.count()  , describe().show() , show(10)\n",
        "    collect(), take(10) \n",
        "DataSet functions\n",
        "     cache\n",
        "     checkpoint\n",
        "    empDF.columns\n",
        "    createGlobalTempView\n",
        "    createorReplaceGlobalTempView\n",
        "    createOrReplaceTempView\n",
        "    createTempView\n",
        "    \n",
        "    Procedure to apply SQL on DataFrames::\n",
        "        empDF.createOrReplaceTempView(\"emp_table\") //inmemory metastore\n",
        "        now empDF can be accessed in sql using a table name emp_table\n",
        "        spark.sql(\"select * from emp_table\") -> returns a DF \n",
        "        how to store values into a concrete table \n",
        "        spark.sql(\"create table xyz as select * from emp_table\") -> returns a DF but unit..\n",
        "        now xyz table is stored in (since not integrated with hive or hdfs) -> stored in local FS with Derby metastire\n",
        "        and can be retrieved later.\n",
        "        2018-10-18 05:17:16 WARN  HiveMetaStore:1383 - \n",
        "        Location: file:/home/dorababugjntup/spark-warehouse/xyz specified for non-external table:xyz\n",
        "rest all DS functions\n",
        "and typed/untyped functions.\n",
        "--------\n",
        "various others sources to read data and create DF/DS\n",
        "\n",
        "trait,object,class,case class\n",
        "privamry vs auxillary consturos (overloaded consturctors)\n",
        "companioin object\n",
        "partial function\n",
        "Inheritance and multiple inheritance in Scala\n",
        "exception handling\n",
        "\n",
        "write standalone scala spark applicaiton and submit to production\n",
        "What is encoders and spark.implicits\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hjvsSkIIUabN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Create DataFrame -> RDD to DF or DS\n",
        "create df/ds from various sources. -> sparkSession.read()\n",
        "1) CSV \n",
        "\n",
        "id,name,sal,deptid\n",
        "1,emp1,2000,dept1\n",
        "2,emp2,6000,dept2\n",
        "3,emp3,500,dept1\n",
        "4,emp4,7000,dept2\n",
        "5,emp5,10000,dept3\n",
        "6,emp6,4000,dept3\n",
        "7,emp7,1000\n",
        "8,8000,dept4\n",
        "\n",
        "spark.read\n",
        "   def read: org.apache.spark.sql.DataFrameReader\n",
        "spark.read.format(\"csv\").load(\"emp_header\")\n",
        "or spark.read.csv(\"emp_header\")\n",
        "\n",
        "\n",
        "val df = sqlContext.read\n",
        "    .format(\"com.databricks.spark.csv\")\n",
        "    .option(\"header\", \"true\") \n",
        "    .option(\"mode\", \"DROPMALFORMED\")\n",
        "    .load(\"csv/file/path\"); \n",
        "    \n",
        "    \n",
        "    \n",
        "    val empDF=spark.read.\n",
        "    format(\"csv\").\n",
        "    option(\"sep\",\",\").\n",
        "    option(\"header\",\"true\").\n",
        "    option(\"inferSchema\",\"true\").\n",
        "    option(\"mode\",\"PERMISSIVE\").\n",
        "    load(\"emp_header\")\n",
        "    empDF.show()\n",
        "    \n",
        "    \n",
        "    import org.apache.spark.sql.types._\n",
        "   val schema1=\n",
        "    StructType(Seq(\n",
        "              StructField(\"id1\",IntegerType,true),\n",
        "               StructField(\"name1\",StringType,true), \n",
        "              StructField(\"sal1\",IntegerType,true), \n",
        "               StructField(\"deptid1\",StringType,true))\n",
        "              )\n",
        "    \n",
        "val empDF=spark.read.\n",
        "    format(\"csv\").\n",
        "    option(\"sep\",\",\").\n",
        "    option(\"header\",\"true\").\n",
        "    option(\"inferSchema\",\"true\").\n",
        "    option(\"mode\",\"PERMISSIVE\").\n",
        "    schema(schema1).\n",
        "    load(\"emp_header\")\n",
        "    \n",
        "    ---------------------------------------\n",
        "    \n",
        "    val empDF=spark.read.\n",
        "    format(\"json\").\n",
        "    option(\"mode\",\"PERMISSIVE\").\n",
        "    load(\"/home/dorababugjntup/YARN/spark/examples/src/main/resources/people.json\")\n",
        "    \n",
        "     empDF.na.drop(1).show\n",
        "atleast one not null value should be there.\n",
        "scala> empDF.na.fill(\"NAN\",Seq(\"gender\")).show\n",
        "scala> empDF.na.replace(List(\"gender\"),Map(\"Male\"->\"N/A\"))\n",
        "\n",
        "Typed oPs::\n",
        "empDF.filter(empDF(\"age\")> 19).show\n",
        "scala> empDF.filter(\"age = 19\").show\n",
        "\n",
        "Untyped Ops\n",
        "2366  empDF.agg(Map(\"age\"->\"max\")).show\n",
        "2367  empDF.agg(Map(\"age\"->\"min\")).show\n",
        "2368  empDF.agg(Map(\"age\"->\"avg\")).show\n",
        "\n",
        "empDF.drop(empDF(\"name\")).show\n",
        "scala> empDF.drop(\"name\").show\n",
        "\n",
        "scala> def test(c1:String, cols:String*)= println(c1+\"***\"+cols.length)\n",
        "test: (c1: String, cols: String*)Unit\n",
        "\n",
        "scala> test(\"a\",\"b\",\"c\")\n",
        "a***2\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RKGpchmKXY02",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Eclipse http://www.eclipse.org/downloads/download.php?file=/technology/epp/downloads/release/2018-09/R/eclipse-dsl-2018-09-win32-x86_64.zip&mirror_id=272\n",
        "  Help->Eclipse MarketPlace\n",
        "  find -> scala -> scala ide 1.7.x\n",
        "  -----------------------------------\n",
        "  next download sbt and set path in windows\n",
        "  mkdir project2\n",
        "  \n",
        "  cd project2\n",
        "  \n",
        "  create build.sbt inside put scalaVersion := \"2.11.11\"\n",
        "    \n",
        "    mkdir project\n",
        "  touch plugin.sbt\n",
        "  \n",
        "    sbt eclipse\n",
        "    now .classpath got created with 2.11\n",
        "    \n",
        "    Change Scala Library Container to 2.11 version\n",
        "    \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AYWZ-LkRxa9L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Date 22 Oct 2018\n",
        "Spark Integration with HDFS and HIVE\n",
        "1) To read data from hdfs and submit jobs to yarn, we need NN and RM address\n",
        "present in site xml files\n",
        "export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
        "2) To Read from Hive Tables, we just need hive metastore details\n",
        "copy hive-site.xml  into YARN/SPARK/conf\n",
        "cp YARN/hive/conf/hive-site.xml YARN/spark/conf/\n",
        "place mysql connector jar driver in spark/jars\n",
        "cp YARN/hive/lib/mysql-connector-java-8.0.11.jar YARN/spark/jars/\n",
        "\n",
        "\n",
        "when using spark and hive with same hive metastore, add below property in hive-site.xml\n",
        "<property>\n",
        "    <name>hive.metastore.schema.verification</name>\n",
        "    <value>false</value>\n",
        "</property>\n",
        "\n",
        "\n",
        "Start hdfs daemons\n",
        "hadoop-daemon.sh start namenode &\n",
        "hadoop-daemon.sh start datanode &\n",
        "hadoop-daemon.sh start secondarynamenode &\n",
        "yarn-daemon.sh start resourcemanager &\n",
        "yarn-daemon.sh start nodemanager &\n",
        "\n",
        "----------------\n",
        "scala>spark.sql(\" sql query \")\n",
        "spark-sql is like hive command\n",
        "spark-sql --help , at the end we get hive cli commands\n",
        "\n",
        "export var=`spark-sql -S -i test.hql -e \"show tables\"`\n",
        "-------------------------------\n",
        "\n",
        "Createting tales in spark-shell access in spark-sql and hive\n",
        "val emp=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"file:///home/dorababugjntup/emp_header\")\n",
        "emp.show()\n",
        "emp.drop(emp(\"deptid\")).show\n",
        "emp.drop(\"id\",\"deptid\").show\n",
        "\n",
        "emp.createOrReplaceTempView(\"emp_table\")\n",
        "\n",
        "Data Frame store into HDFS::::\n",
        "--------------------------\n",
        "emp.write. \n",
        "bucketBy   format       jdbc   mode     options   parquet       save          sortBy\n",
        "csv        insertInto   json   option   orc       partitionBy   saveAsTable   text\n",
        "\n",
        "csv,orc,json,jdbc,saveAsTable hive table name\n",
        "emp.write.format(\"csv\").save(\"/emp_csv_spark\")\n",
        "\n",
        "emp.write\n",
        "   def write: org.apache.spark.sql.DataFrameWriter[org.apache.spark.sql.Row]\n",
        "emp.write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").save(\"/emp_csv_spark\")\n",
        "emp.write.format(\"csv\").option(\"header\",\"true\").option(\"sep\",\"#\").mode(\"append\").save(\"/emp_csv_spark\")\n",
        "emp.write.format(\"csv\").option(\"header\",\"true\").option(\"sep\",\"#\").mode(\"overwrite\").partitionBy(\"deptid\").save(\"/emp_csv_spark\")\n",
        "        \n",
        "scala> emp.write.saveAsTable(\"default.emp\")\n",
        "for Hive it istaking ORC as default\n",
        "\n",
        "scala> emp.write.save(\"/emp_spark\")\n",
        "for spark defult file format is PARQUET\n",
        "hdfs dfs -cat /emp_spark/*   give parquet\n",
        "val emp1=spark.read.load(\"/emp_spark\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OvgLvqWpFwiS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "download spark in windows\n",
        "download scala in windows, extract and set SCALA_HOME and PATH\n",
        "download sbt  in windows extract and set SBT_HOME and path\n",
        "jdk download::::\n",
        "http://download.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-windows-x64.exe\n",
        "download GIT\n",
        "Eclipse http://www.eclipse.org/downloads/download.php?file=/technology/epp/downloads/release/2018-09/R/eclipse-dsl-2018-09-win32-x86_64.zip&mirror_id=272\n",
        "  Extract , open the eclipse \n",
        "  Help->Eclipse MarketPlace\n",
        "  \n",
        "  find -> scala -> scala ide 1.7.x  -> install , this creates scala SDK in eclipse\n",
        "  -----------------------------------\n",
        "  \n",
        "  \n",
        "  eclipse-java-oxygen-3a-win32-x86_64 and scala SDK working in my system\n",
        "  http://www.eclipse.org/downloads/download.php?file=/technology/epp/downloads/release/oxygen/3a/eclipse-dsl-oxygen-3a-win32-x86_64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dd7jSq6L-07F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "24 OCT 2018\n",
        "\n",
        "https://dorababu-bigdata-trainer.blogspot.com/2018/10/git-essentials.html\n",
        "https://github.com/datamitra/scala-code/blob/master/ScalaDemo/src/HelloWorld.scala\n",
        "\n",
        "Create Eclipse projects -> prject structure ??\n",
        "SBT  file reading scala code\n",
        "\n",
        "\n",
        "compoile package ,run the jar file supply file name arguments -> print file content\n",
        "\n",
        "--------\n",
        "MAVEN  scala project \n",
        "-----------\n",
        "GIT in windows make the code availbe in GCP \n",
        "---------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2AR-cdwSXwZf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir SparkDemo\n",
        "build.sbt\n",
        "\n",
        "scalaVersion := \"2.11.11\"\n",
        "  \n",
        "  echo scalaVersion := \"2.11.11\">build.sbt\n",
        "  \n",
        "  \n",
        "  Use sbt version 0.13\n",
        "  C:\\Users\\username\\.sbt\\0.13\\plugins\\plugins.sbt\n",
        "    addSbtPlugin(\"com.typesafe.sbteclipse\" % \"sbteclipse-plugin\" % \"3.0.0\")\n",
        "\n",
        "  https://github.com/sbt/sbteclipse\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "keCj2n7fss8b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "build.sbt\n",
        "\n",
        "name := \"Simple Project\"\n",
        "\n",
        "version := \"1.0\"\n",
        "\n",
        "scalaVersion := \"2.11.11\"\n",
        "\n",
        "libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.3.2\"\n",
        "libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.3.2\"\n",
        "libraryDependencies += \"org.apache.spark\" %% \"spark-hive\" % \"2.3.2\" % \"provided\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lojXJlF1t4xW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "https://alvinalexander.com/source-code/scala/common-sbt-commands-scala-sbt\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ABtYFAwn1GBT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "package com.bdt\n",
        "\n",
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.SparkConf\n",
        "import org.apache.spark.SparkContext\n",
        "\n",
        "\n",
        "object SparkDemo {\n",
        "  def main(args: Array[String]): Unit = {\n",
        "    \n",
        "  \n",
        "  val conf=new SparkConf();\n",
        "  val spark=SparkSession\n",
        "  .builder()\n",
        "  .appName(\"Sample Spark\")\n",
        "  .config(conf)\n",
        "  .config(\"spark.app.name\",\"Spark Demo \")\n",
        "  .master(\"local[*]\")\n",
        "    .getOrCreate()\n",
        "  \n",
        "  val sc=spark.sparkContext\n",
        "  \n",
        "  val df1=spark.range(100)\n",
        "  df1.show(5)\n",
        "  }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-CZCcN3QK7zV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Maven Spark Project\n",
        "https://mvnrepository.com/artifact/org.scala-lang/scala-library/2.11.11\n",
        "https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.11/2.2.1\n",
        "https://mvnrepository.com/artifact/org.apache.spark/spark-sql_2.11/2.2.1\n",
        "https://mvnrepository.com/artifact/org.apache.spark/spark-streaming_2.11/2.2.1\n",
        "  \n",
        "  \n",
        "  <!-- https://mvnrepository.com/artifact/org.scala-lang/scala-library -->\n",
        "<dependency>\n",
        "    <groupId>org.scala-lang</groupId>\n",
        "    <artifactId>scala-library</artifactId>\n",
        "    <version>2.11.11</version>\n",
        "</dependency>\n",
        "\n",
        "<!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->\n",
        "<dependency>\n",
        "    <groupId>org.apache.spark</groupId>\n",
        "    <artifactId>spark-core_2.11</artifactId>\n",
        "    <version>2.2.1</version>\n",
        "</dependency>\n",
        "\n",
        "     <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql -->\n",
        "<dependency>\n",
        "    <groupId>org.apache.spark</groupId>\n",
        "    <artifactId>spark-sql_2.11</artifactId>\n",
        "    <version>2.2.1</version>\n",
        "</dependency>\n",
        "    \n",
        "    <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming -->\n",
        "<dependency>\n",
        "    <groupId>org.apache.spark</groupId>\n",
        "    <artifactId>spark-streaming_2.11</artifactId>\n",
        "    <version>2.2.1</version>\n",
        "    <scope>provided</scope>\n",
        "</dependency>\n",
        " \n",
        "  \n",
        "  Scala Maven plugin::\n",
        "  \n",
        "  http://scala-ide.org/docs/tutorials/m2eclipse/\n",
        "    \n",
        "    Help Install new software::\n",
        "    Maven Scala - http://alchim31.free.fr/m2e-scala/update-site\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jgEPrP0L7KA2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6TvpD_VrN4sS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spark-submit  \\\n",
        "--master yarn \\\n",
        "--deploy-mode cluster \\\n",
        "--executor-memory 1g \\\n",
        "--driver-memory 1g \\\n",
        "--name SparkDEMOCLIENTMODE \\\n",
        "--class com.bdt.SparkDemo \\\n",
        "/home/dorababugjntup/scala-code/SparkDemo.jar /home/dorababugjntup/emp.txt\n",
        "\n",
        " \n",
        "  \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6WA8fTkFN4ft",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fIEJqjqRN76T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stepno,Cache,Broadcat,temptable?,temptable_name,sqlfilename\n",
        "1,Y,N,Y,emp_temp,folder/step1.hql   -> select * from \n",
        "2,N,Y,,,folder/step2.hql -> insert ovewrite tbale   from ... emp_temp\n",
        "\n",
        "\n",
        "\n",
        "scala code first read the csv file\n",
        "\n",
        "read line by line\n",
        "open file step1.hql\n",
        "\n",
        "if(temptable?)\n",
        "val df=spark.sql(step1.hql)\n",
        " if broadcast??\n",
        "\tsc.boradcast(df)\n",
        " if cache?\n",
        "    df.cache()\n",
        "df.createOrReplacevieww(emp_temp)\n",
        "else \n",
        " spark.sql(step2.hql)\n",
        " ------------------\n",
        " jarfile   csvfilename   folder \n",
        " \n",
        " folder->\n",
        " csvfile\n",
        " step1.hql\n",
        " step2.hql\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "762NI7RZZr0Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "FLUME INSTALLATION - 1 Nov 2018\n",
        "\n",
        "export FLUME_HOME=$HOME/YARN/flume\n",
        "export FLUME_CONF_DIR=$FLUME_HOME/conf\n",
        "export FLUME_CLASSPATH=$FLUME_CONF_DIR\n",
        "export PATH=$PATH:$FLUME_HOME/bin\n",
        "\n",
        "  \n",
        "  wget link to software\n",
        "  tar -xvf apache-flume-1.8.0-bin.tar.gz\n",
        " mv apache-flume-1.8.0-bin YARN/flume\n",
        "\n",
        "vi .bashrc\n",
        "souce .bashrc\n",
        "which flume-ng\n",
        "\n",
        "\n",
        "echo 111111111 >> server.log\n",
        "tail -f server.log\n",
        "\n",
        "\n",
        "vi exec_agent.conf\n",
        "\n",
        "flmagent.sources=exec-source\n",
        "flmagent.sinks=hdfs-sink\n",
        "flmagent.channels=ch1\n",
        "\n",
        "\n",
        "flmagent.sources.exec-source.type = exec\n",
        "flmagent.sources.exec-source.command = tail -F server.log\n",
        "flmagent.sources.exec-source.channels = ch1\n",
        "\n",
        "\n",
        "\n",
        "flmagent.sinks.hdfs-sink.type=hdfs\n",
        "flmagent.sinks.hdfs-sink.hdfs.path= hdfs://localhost:9000/flumeprog/execflume/\n",
        "flmagent.sinks.hdfs-sink.hdfs.filePrefix=apacheaccess\n",
        "flmagent.sinks.hdfs-sink.hdfs.rollInterval=10\n",
        "flmagent.sinks.hdfs-sink.hdfs.rollSize=0\n",
        "flmagent.sinks.hdfs-sink.hdfs.writeFormat=Text\n",
        "flmagent.sinks.hdfs-sink.channel=ch1\n",
        "\n",
        "flmagent.channels.ch1.type=memory\n",
        "flmagent.channels.ch1.capacity=1000\n",
        "\n",
        "\n",
        "\n",
        "--------------\n",
        "flume-ng agent --conf $FLUME_HOME/conf \\\n",
        "--name flmagent --conf-file $HOME/exec_agent.conf \\\n",
        "-Dflume.root.logger=DEBUG,console\n",
        "\n",
        "\n",
        " hdfs dfs  -text /flumeprog/execflume/*\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e7jJsYbsmPZO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "https://github.com/dorababugjntup/SET2\n",
        "https://github.com/datamitra/scala-code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WgGRoR24a_pO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PIG and PIG LATIN - 2Nov 2018\n",
        "Download pig::\n",
        "wget https://www-eu.apache.org/dist/pig/latest/pig-0.17.0.tar.gz\n",
        "  tar -xvf pig-0.17.0.tar.gz\n",
        "  mv pig-0.17.0 YARN/pig\n",
        "  \n",
        "  vi .bashrc\n",
        "  export PIG_HOME=$HOME/YARN/pig\n",
        "  export PATH=$PATH:$PIG_HOME/bin\n",
        "  \n",
        "  class code\n",
        "  wget https://github.com/dorababugjntup/SET2/raw/master/Pig.tar.gz\n",
        "  \n",
        "  cd $HOME/Pig/DataSet-Pig\n",
        "\n",
        "    \n",
        "    pig -x local\n",
        "    pig \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    divs = LOAD 'NYSE_dividends' ; \n",
        "\n",
        "    grunt>     divs = LOAD 'NYSE_dividends'  using PigStorage('\\t')\n",
        "    store divs into 'out' using PigStorage('|')\n",
        "describe divs;\n",
        "     divs1= foreach divs generate $1,$2;\n",
        "  \n",
        "  grunt>     divs = LOAD 'NYSE_dividends'  using PigStorage('\\t') as (f1,f2,f3,f4,f5,f6);\n",
        "\n",
        "  divs: {f1: bytearray,f2: bytearray,f3: bytearray,f4: bytearray,f5: bytearray,f6: bytearray}\n",
        "grunt> divs1= foreach divs generate $2,f1,$1;\n",
        "grunt> divs1= foreach divs generate $2,LOWER(f1) as lw,$1;\n",
        "divs1: {f3: bytearray,lw: chararray,f2: bytearray}\n",
        "\n",
        "divs = LOAD 'NYSE_dividends' using PigStorage('\\t') as (exchange:chararray, symbol:chararray, date:chararray, dividends:float) ;\n",
        "divs1= foreach divs generate .. $2;\n",
        " divs1 = foreach divs generate UPPER(symbol) as symbol,$2 ;\n",
        "divs_cyn = filter divs by symbol=='CYN';\n",
        "\n",
        "\n",
        "every company how much total dividend given? \n",
        "group and apply sum , avg\n",
        "\n",
        "\n",
        "grunt> grpd = group divs by symbol;\n",
        "grunt> describe grpd;\n",
        "grpd: {group: chararray,divs: {(exchange: chararray,symbol: chararray,date: chararray,dividends: float)}}\n",
        "grunt> divs_sum= foreach grpd generate group ,SUM(divs.dividends) as tot_div;\n",
        "grunt> describe divs_sum;\n",
        "divs_sum: {group: chararray,tot_div: double}\n",
        "\n",
        "  \n",
        "  daily = load 'NYSE_daily' as (exchange, symbol, date, open, high, low, close, volume, adj_close);\n",
        "  jnd = join divs by (symbol,date) , daily by (symbol,date)\n",
        "jnd: {divs::exchange: chararray,divs::symbol: chararray,divs::date: chararray,divs::dividends: float,\n",
        "      daily::exchange: bytearray,daily::symbol: bytearray,daily::date: bytearray,\n",
        "      daily::open: bytearray,daily::high: bytearray,daily::low: bytearray,\n",
        "      daily::close: bytearray,daily::volume: bytearray,daily::adj_close: bytearray}\n",
        "\n",
        "  res = foreach jnd generate divs::exchange as exchange, divs::symbol as symbol,divs::date as date,dividends,open,volume,high;\n",
        "  res: {divs::exchange: chararray,divs::symbol: chararray,divs::date: chararray,divs::dividends: float,daily::open: bytearray,daily::volume: bytearray,daily::high: bytearray}\n",
        "  res: {exchange: chararray,symbol: chararray,date: chararray,divs::dividends: float,daily::open: bytearray,daily::volume: bytearray,daily::high: bytearray}\n",
        "\n",
        "   res_cyn= filter res by symbol=='CYN';\n",
        "  \n",
        "  \n",
        "  grunt > exec file.pig  -> commands doesnt come in history, also aliases doesnt come in memory\n",
        "  grunt> run file.pig -> commnds and aliases in the file will be available.\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  23   Z = cogroup A BY $0, B BY $0;\n",
        "24   Z = cogroup A BY $0 inner, B BY $0;\n",
        "25   Z = cogroup A BY $0 inner, B BY $0 inner;\n",
        "20   A = LOAD 'data1' AS (a1:int,a2:int,a3:int);\n",
        "21   B = LOAD 'data2' AS (b1:int,b2:int);\n",
        "22   Z = JOIN A BY $0, B BY $0;\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "krsVQ9TgNL2w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "HBASE      6-Nov-2018\n",
        "\n",
        "NOTE::: Download HBASE 1.x version , 2.x may have conflict of ports with GCP Linux\n",
        "wget https://www-us.apache.org/dist/hbase/2.1.1/hbase-2.1.1-bin.tar.gz\n",
        "tar -xvf hbase-2.1.1-bin.tar.gz\n",
        "mv hbase-2.1.1 YARN/hbase \n",
        "vi .bashrc\n",
        "export HBASE_HOME=$HOME/YARN/hbase\n",
        "export PATH=$PATH:$HBASE_HOME/bin\n",
        "source .bashrc\n",
        "cd $HOME/YARN/hbase/conf\n",
        "echo \"export JAVA_HOME=$JAVA_HOME\" >>  hbase-env.sh\n",
        "mkdir $HBASE_HOME/data\n",
        "\n",
        "vi hbase-site.xml     keep this inside <configuration>\n",
        "\n",
        "<configuration> \n",
        "  <property><name>hbase.tmp.dir</name><value>file://${user.home}/YARN/hbase/data</value></property> \n",
        "<!-- <property> <name>hbase.zookeeper.property.clientPort</name><value>2182</value></property> \n",
        " <property><name>hbase.rootdir</name><value>file://${user.home}/YARN/hbase/hbaseroot</value></property>\n",
        " <property><name>hbase.zookeeper.property.dataDir</name><value>file://${user.home}/YARN/hbase//zookeeper</value>  </property>    \n",
        "  -->    \n",
        "  </configuration>\n",
        " \n",
        "  \n",
        "  start-hbase.sh     ;;    jps ;;   hbase shell\n",
        "  \n",
        "  \n",
        "  \n",
        "create 'test','data'\n",
        "list\n",
        "scan 'test'\n",
        " put 'test', 'r1', 'data:col1', 'val1'\n",
        " put 'test', 'r2', 'data:col2', 'val2'\n",
        " put 'test', 'r3', 'data:col3', 'val3'\n",
        "\n",
        "\n",
        "hbase(main):019:0> scan 'test'\n",
        "ROW                          COLUMN+CELL\n",
        " r1                          column=data:col1, timestamp=1541479725362, value=ZZZ\n",
        " r2                          column=data:col2, timestamp=1541479677841, value=val2\n",
        " r3                          column=data:col3, timestamp=1541479693149, value=val3\n",
        "3 row(s)\n",
        "Took 0.0100 seconds\n",
        "hbase(main):020:0> get 'test','r1','data:col1'\n",
        "COLUMN                       CELL\n",
        " data:col1                   timestamp=1541479725362, value=ZZZ\n",
        "1 row(s)\n",
        "Took 0.0447 seconds\n",
        "  hbase> t.get 'r1', {COLUMN => 'c1', TIMESTAMP => ts1}\n",
        "get 'test','r1',{ COLUMN =>'data:col1', TIMESTAMP => 1541479615434}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M3YKT_5kk9aj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FTfYJedKk9Nj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZkONox-uk89v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PROJECT::: PART 1 - Data ingestion\n",
        "      wget https://github.com/datamitra/notes/raw/master/project/data/mysqlsampledatabase.sql\n",
        "      insert data into mysql\n",
        "      \n",
        "(all code editing in windows , migrate the code into GCP using GITHUB/BITBUCKET)\n",
        "      \n",
        "Data base names and table naming :::\n",
        "\n",
        "mysql - tables\n",
        "mysql control table -> CNTRL - job_run_status\n",
        "(sqoop_table_name,status(P/E/R/C),extracted_date,max_modified_date_mysql_table)\n",
        "\n",
        "Hive - incremental database - stg_datalake\n",
        "Hive - main table database - hdp_datalake\n",
        "The data type is incremental last modified (Mutable Data)\n",
        "------------------\n",
        "Perform incremental logic using hql , Spark-sql\n",
        "-------------------------\n",
        "\n",
        "For one of the table we need to get data in real time.\n",
        "into hdfs folder.\n",
        "source table -> javap->kafka->flume -> hdfs \n",
        "flume use spark dstreams/spark structured streaming\n",
        "(use spark-submit)\n",
        "\n",
        "Use Spooling directory and spark-streaming \n",
        "to load csv file into hdfs table\n",
        "--------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jQ536E-hKE2x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-kqFgF0qm2BR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GIT SETUP\n",
        "login to github /bitbucket\n",
        "create repository - Project_June18_WD_Morning\n",
        "\n",
        "In Windows - install git\n",
        "Create one folder , run the below command\n",
        "git clone https://github.com/datamitra/Project_June18_WD_Morning.git\n",
        "  \n",
        "In GCP - in Home direcory \n",
        "git clone https://github.com/datamitra/Project_June18_WD_Morning.git\n",
        "  \n",
        "Test:: create one file in windows, get the data in GCP\n",
        "\n",
        " in Windows -\n",
        "git add *     #add only changed files in production\n",
        "git commi -m \"some message \"\n",
        "git push origin master\n",
        "git config user.email \"datamitra1@gmail.com\"\n",
        "git config user.name \"datamitra1\"\n",
        " \n",
        "GCP: go inside the project folder\n",
        "  git pull\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4IVsCVY0tyOI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Load Source data into Mysql\n",
        "wget https://github.com/datamitra/Project_June18_WD_Morning/raw/master/data/mysqlsampledatabase.zip\n",
        "unzip mysqlsampledatabase.zip\n",
        "\n",
        "mysql -uroot -proot\n",
        "mysql>show databases;\n",
        "mysql> source mysqlsampledatabase.sql\n",
        "show databases;\n",
        "use classicmodels;\n",
        "show tables;\n",
        "describe each table.\n",
        "mysql> show tables;\n",
        "+-------------------------+\n",
        "| Tables_in_classicmodels |\n",
        "+-------------------------+\n",
        "| customers               |\n",
        "| employees               |\n",
        "| offices                 |\n",
        "| orderdetails            |\n",
        "| orders                  |\n",
        "| payments                |\n",
        "| productlines            |\n",
        "| products                |\n",
        "+-------------------------+\n",
        "8 rows in set (0.00 sec)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iMHKrMc4xzyk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "1) Create hive tbale structure and perform full refresh once\n",
        "2) leave lob(binary lob and clob) columns - for this you need to select few columns alone\n",
        "(create tables in incr database and main database)\n",
        "\n",
        "3) make the script reusable\n",
        "Once you create hive table structure\n",
        "write import sqoop for incremental last modified\n",
        "while peforming incr refresh - update contol table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nnO3gSCU0qVz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Real time streaming\n",
        "table -> java producer( use the control for max last modifed) \n",
        "kafka -> flue/spark streaming into hive table.\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}